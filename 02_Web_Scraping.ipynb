{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3c948a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting requests\n",
      "  Downloading requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting beautifulsoup4\n",
      "  Downloading beautifulsoup4-4.14.2-py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: pandas in c:\\users\\user\\miniconda3\\envs\\ml_cours\\lib\\site-packages (2.3.3)\n",
      "Collecting charset_normalizer<4,>=2 (from requests)\n",
      "  Downloading charset_normalizer-3.4.4-cp313-cp313-win_amd64.whl.metadata (38 kB)\n",
      "Collecting idna<4,>=2.5 (from requests)\n",
      "  Downloading idna-3.11-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests)\n",
      "  Using cached urllib3-2.5.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests)\n",
      "  Downloading certifi-2025.11.12-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4)\n",
      "  Downloading soupsieve-2.8-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\user\\miniconda3\\envs\\ml_cours\\lib\\site-packages (from beautifulsoup4) (4.15.0)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\user\\miniconda3\\envs\\ml_cours\\lib\\site-packages (from pandas) (2.3.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\user\\miniconda3\\envs\\ml_cours\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\user\\miniconda3\\envs\\ml_cours\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\user\\miniconda3\\envs\\ml_cours\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\miniconda3\\envs\\ml_cours\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Downloading requests-2.32.5-py3-none-any.whl (64 kB)\n",
      "Downloading charset_normalizer-3.4.4-cp313-cp313-win_amd64.whl (107 kB)\n",
      "Downloading idna-3.11-py3-none-any.whl (71 kB)\n",
      "Using cached urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
      "Downloading beautifulsoup4-4.14.2-py3-none-any.whl (106 kB)\n",
      "Downloading certifi-2025.11.12-py3-none-any.whl (159 kB)\n",
      "Downloading soupsieve-2.8-py3-none-any.whl (36 kB)\n",
      "Installing collected packages: urllib3, soupsieve, idna, charset_normalizer, certifi, requests, beautifulsoup4\n",
      "\n",
      "   ---------------------------------------- 0/7 [urllib3]\n",
      "   ----------- ---------------------------- 2/7 [idna]\n",
      "   ----------------- ---------------------- 3/7 [charset_normalizer]\n",
      "   ---------------------------- ----------- 5/7 [requests]\n",
      "   ---------------------------------------- 7/7 [beautifulsoup4]\n",
      "\n",
      "Successfully installed beautifulsoup4-4.14.2 certifi-2025.11.12 charset_normalizer-3.4.4 idna-3.11 requests-2.32.5 soupsieve-2.8 urllib3-2.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip install requests beautifulsoup4 pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48809cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60ac36f",
   "metadata": {},
   "source": [
    "# Fct  de scrapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49308bda",
   "metadata": {},
   "source": [
    "#TODO : to be updated\n",
    "\n",
    "def scrape_reviews(url, num_pages=1):\n",
    "    all_reviews = []\n",
    "    for page in range(1, num_pages + 1):\n",
    "        page_url = f\"{url}?page={page}\" # Adapter l'URL de pagination\n",
    "        print(f\"Scraping page: {page_url}\")\n",
    "        try:\n",
    "            response = requests.get(page_url, headers={'User-Agent': 'Mozilla/5.0'})\n",
    "            response.raise_for_status() # LÃ¨ve une exception pour les codes d'Ã©tat HTTP d'erreur\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "            # Exemple de sÃ©lecteurs - Ã€ ADAPTER ABSOLUMENT Ã  la structure du site cible\n",
    "            review_containers = soup.find_all('div', class_='review-item') # Ex: div avec classe 'review-item'\n",
    "\n",
    "            for container in review_containers:\n",
    "                review_text = container.find('p', class_='review-text').get_text(strip=True) if container.find('p', class_='review-text') else \"N/A\"\n",
    "                rating = container.find('span', class_='rating').get_text(strip=True) if container.find('span', class_='rating') else \"N/A\"\n",
    "                # Ajoutez d'autres champs comme date, auteur, etc.\n",
    "\n",
    "                all_reviews.append({\n",
    "                    'review_text': review_text,\n",
    "                    'rating': rating\n",
    "                })\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Erreur lors de la requÃªte vers {page_url}: {e}\")\n",
    "        time.sleep(2) # Soyez poli avec le serveur, attendez un peu entre les requÃªtes\n",
    "\n",
    "    return pd.DataFrame(all_reviews)\n",
    "\n",
    "# Exemple d'utilisation (URL Ã  remplacer par celle du site tunisien cible)\n",
    "# url_cible = \"https://www.mytek.tn/smartphone-samsung-galaxy-a54-5g-8-256go.html#reviews\"\n",
    "# df_reviews = scrape_reviews(url_cible, num_pages=5)\n",
    "# df_reviews.to_csv('raw_customer_reviews.csv', index=False)\n",
    "# print(df_reviews.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69e54949",
   "metadata": {},
   "source": [
    "#chat version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac0b21e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âŒ Aucun avis trouvÃ© sur cette page.\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n",
      "âœ”ï¸ CSV enregistrÃ© \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_tunisianet_reviews(product_url):\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\"\n",
    "    }\n",
    "\n",
    "    response = requests.get(product_url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    # Bloc contenant les avis\n",
    "    reviews_section = soup.find(\"div\", id=\"product_reviews_block\")\n",
    "\n",
    "    if not reviews_section:\n",
    "        print(\"âŒ Aucun avis trouvÃ© sur cette page.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    all_reviews = reviews_section.find_all(\"div\", class_=\"review-item\")\n",
    "    \n",
    "    data = []\n",
    "\n",
    "    for review in all_reviews:\n",
    "\n",
    "        # Nom du client\n",
    "        name_tag = review.find(\"span\", class_=\"reviewer-name\")\n",
    "        name = name_tag.text.strip() if name_tag else \"Inconnu\"\n",
    "\n",
    "        # Date de l'avis\n",
    "        date_tag = review.find(\"span\", class_=\"review-date\")\n",
    "        date = date_tag.text.strip() if date_tag else \"Inconnue\"\n",
    "\n",
    "        # Note (nombre dâ€™Ã©toiles)\n",
    "        star_container = review.find(\"div\", class_=\"star_content\")\n",
    "        rating = len(star_container.find_all(\"i\", class_=\"fa fa-star\")) if star_container else None\n",
    "\n",
    "        # Commentaire\n",
    "        comment_tag = review.find(\"p\", class_=\"review-text\")\n",
    "        comment = comment_tag.text.strip() if comment_tag else \"\"\n",
    "\n",
    "        # Sauvegarde\n",
    "        data.append({\n",
    "            \"name\": name,\n",
    "            \"date\": date,\n",
    "            \"rating\": rating,\n",
    "            \"comment\": comment\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "\n",
    "# -------------------------------\n",
    "# ðŸ”¥ Tester avec le Redmi Note 13\n",
    "# -------------------------------\n",
    "\n",
    "#url = \"https://www.tunisianet.com.tn/smartphone/71310-smartphone-xiaomi-redmi-note-13-8-go-256-go-noir.html\"\n",
    "url = \"https://www.tunisianet.com.tn/smartphone/71347-smartphone-samsung-galaxy-a54-5g-128-go-8-go-ram-lime.html\"\n",
    "\n",
    "df = scrape_tunisianet_reviews(url)\n",
    "\n",
    "print(df)\n",
    "\n",
    "df.to_csv(\"samsung_a54.csv\", index=False)\n",
    "print(\"âœ”ï¸ CSV enregistrÃ© \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9ee7062",
   "metadata": {},
   "source": [
    "Code de scraping pour Orange Tunisie (HTML statique)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ac5d512f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âŒ Aucun avis trouvÃ©.\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n",
      "âœ… CSV sauvegardÃ©.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "def scrape_orange_reviews(product_url):\n",
    "    headers = {\n",
    "        \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64)\"\n",
    "    }\n",
    "\n",
    "    response = requests.get(product_url, headers=headers)\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    # Section des avis\n",
    "    review_blocks = soup.find_all(\"div\", class_=\"review\")  # Ã  adapter selon le site\n",
    "\n",
    "    if not review_blocks:\n",
    "        print(\"âŒ Aucun avis trouvÃ©.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    data = []\n",
    "\n",
    "    for review in review_blocks:\n",
    "        # Nom\n",
    "        name_tag = review.find(\"span\", class_=\"reviewer-name\")\n",
    "        name = name_tag.text.strip() if name_tag else \"Inconnu\"\n",
    "\n",
    "        # Note\n",
    "        rating_tag = review.find(\"div\", class_=\"stars\")  # vÃ©rifier le HTML exact\n",
    "        rating = rating_tag[\"data-rating\"] if rating_tag else None\n",
    "\n",
    "        # Commentaire\n",
    "        comment_tag = review.find(\"p\", class_=\"review-text\")\n",
    "        comment = comment_tag.text.strip() if comment_tag else \"\"\n",
    "\n",
    "        data.append({\n",
    "            \"name\": name,\n",
    "            \"rating\": rating,\n",
    "            \"comment\": comment\n",
    "        })\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    return df\n",
    "\n",
    "# -------------------------\n",
    "# Tester\n",
    "# -------------------------\n",
    "url = \"https://www.orange.tn/fr/smartphones/samsung-galaxy-a54\"\n",
    "df = scrape_orange_reviews(url)\n",
    "print(df.head())\n",
    "\n",
    "df.to_csv(\"reviews_orange_galaxy_a54.csv\", index=False)\n",
    "print(\"âœ… CSV sauvegardÃ©.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71eea01f",
   "metadata": {},
   "source": [
    "# =>\n",
    "Le problÃ¨me rencontrÃ© avec Beautiful Soup est typique car de nombreux sites, y compris ceux comme Orange et Ooredoo en Tunisie, masquent les avis clients via des techniques dynamiques (JavaScript, chargement asynchrone, donnÃ©es JSON cachÃ©es) qui ne sont pas accessibles par un simple parsing HTML."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a118b613",
   "metadata": {},
   "source": [
    "Solutions pour contourner ce problÃ¨me\n",
    "Extraction via donnÃ©es JSON cachÃ©es : Certains sites stockent les avis dans des structures JSON invisibles directement dans le code source, qui peuvent Ãªtre extraites aprÃ¨s avoir identifiÃ© les bons endpoints ou structures. Par exemple, des sites comme Tripadvisor utilisent cette mÃ©thode pour leurs avis clients.\n",
    "\n",
    "Scraping dynamique avec Selenium ou Puppeteer : Ces outils simulent un navigateur complet qui exÃ©cute le JavaScript afin de charger entiÃ¨rement la page et ses avis avant extraction.\n",
    "\n",
    "Utilisation d'APIs ou sources alternatives : Parfois, les plateformes ont des APIs publiques ou semi-publiques pour accÃ©der aux avis clients, ou bien on peut extraire les avis depuis des sites tiers comme Trustpilot qui rÃ©fÃ©rencent les avis des clients pour ces opÃ©rateurs.\n",
    "\n",
    "Automatisation via plateformes comme Make.com + Apify : Certaines plateformes proposent des modules dÃ©jÃ  prÃªts pour collecter des avis de sites comme Trustpilot, avec des options d'analyse, ce qui pourrait faciliter la collecte.\n",
    "\n",
    "Exemples spÃ©cifiques aux opÃ©rateurs en Tunisie\n",
    "Avis pour Orange Tunisie peuvent Ãªtre trouvÃ©s sur Tripadvisor et Trustpilot mais pas facilement scrapÃ©s via BeautifulSoup Ã  cause du contenu dynamique.â€‹\n",
    "\n",
    "Avis pour Ooredoo Tunisie existent aussi sur Trustpilot et Indeed mais de mÃªme, nÃ©cessitent des mÃ©thodes avancÃ©es de scraping ou API.â€‹\n",
    "\n",
    "Tutoriels sur le scraping d'avis clients montrent que juste BeautifulSoup est souvent insuffisant pour scraper tous les avis car il faut gÃ©rer la partie JavaScript ou JSON cachÃ©e.â€‹\n",
    "\n",
    "Pour notre mini-projet on prefere opter pour  : notamment Selenium pour le scraping dynamique, ou bien l'extraction directe de JSON cachÃ©s, ou encore d'utiliser des sources comme Trustpilot pour l'analyse des avis clients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b659bf33",
   "metadata": {},
   "source": [
    "option 1:Selenium\n",
    "\n",
    "Simule un vrai navigateur\n",
    "\n",
    "Charge le JS â†’ les avis deviennent visibles\n",
    "\n",
    "Compatible avec Tunisianet / Orange / Ooredoo\n",
    "\n",
    "Cette mÃ©thode est la plus sÃ»re pour scraper des avis en Tunisie aujourdâ€™hui\n",
    "\n",
    "\n",
    "\n",
    "Option 2 : Extraire des avis depuis les rÃ©seaux sociaux ou forums\n",
    "\n",
    "Facebook, Twitter, forums de tech tunisiens\n",
    "\n",
    "Souvent visibles sans JS compliquÃ©\n",
    "\n",
    "Mais nÃ©cessite un peu de nettoyage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059e54ba",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1fde2562",
   "metadata": {},
   "source": [
    "I WILL WORK ON BUSINESS HOTEL\n",
    "\n",
    "MAPS:\n",
    "\n",
    "https://www.google.com/maps/place/Business+H%C3%B4tel/@36.818325,10.1820211,17z/data=!4m8!3m7!1s0x12fd3489b5a4f4e1:0xdda07f445b5b03cd!8m2!3d36.818325!4d10.184596!9m1!1b1!16s%2Fg%2F11bwn563xh?entry=ttu&g_ep=EgoyMDI1MTEyMy4xIKXMDSoASAFQAw%3D%3D\n",
    "\n",
    "\n",
    "avis dans site de booking:\n",
    "https://www.booking.com/reviews/tn/hotel/business.fr.html?label=gen173nr-10CA0o4gFCCGJ1c2luZXNzSDNYBGjiAYgBAZgBM7gBF8gBDNgBA-gBAfgBAYgCAagCAbgCl86gyQbAAgHSAiRlZjRhOGM4ZS1hNzA0LTRkYzItYjIyNC1hNWMxMjk2YWI0YmXYAgHgAgE&sid=32b2429f18b4624fc09d32f51f4441cc&customer_type=total&hp_nav=0&keep_landing=1&order=featuredreviews&page=1&r_lang=fr&rows=75&"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67c60ed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to fetch TripAdvisor page 1\n",
      "Failed to fetch TripAdvisor page 2\n",
      "No reviews to save.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "# Function to scrape Booking.com reviews\n",
    "def scrape_booking(url_base, pages=2):\n",
    "    reviews = []\n",
    "    for page in range(1, pages + 1):\n",
    "        url = f\"{url_base}&page={page}\"\n",
    "        response = requests.get(url)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Failed to fetch Booking.com page {page}\")\n",
    "            continue\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        review_blocks = soup.find_all('div', class_='review_list_new_item_block')\n",
    "        for block in review_blocks:\n",
    "            try:\n",
    "                reviewer = block.find('span', class_='bui-avatar-block__title').text.strip() if block.find('span', class_='bui-avatar-block__title') else 'Anonymous'\n",
    "                date = block.find('p', class_='review_item_date').text.strip() if block.find('p', class_='review_item_date') else 'N/A'\n",
    "                rating = block.find('div', class_='bui-review-score__badge').text.strip() if block.find('div', class_='bui-review-score__badge') else 'N/A'\n",
    "                title = block.find('h3', class_='c-review-block__title').text.strip() if block.find('h3', class_='c-review-block__title') else 'No Title'\n",
    "                text_elements = block.find_all('span', class_='c-review__body')\n",
    "                text = ' '.join([elem.text.strip() for elem in text_elements]) if text_elements else 'No Text'\n",
    "                reviews.append({\n",
    "                    'source': 'Booking.com',\n",
    "                    'reviewer': reviewer,\n",
    "                    'date': date,\n",
    "                    'rating': rating,\n",
    "                    'title': title,\n",
    "                    'text': text\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"Error parsing Booking.com review: {e}\")\n",
    "    return reviews\n",
    "\n",
    "# Function to scrape TripAdvisor reviews\n",
    "def scrape_tripadvisor(url_base, pages=2):\n",
    "    reviews = []\n",
    "    for page in range(0, pages * 10, 10):  # TripAdvisor uses offsets like or10, or20\n",
    "        url = url_base if page == 0 else f\"{url_base}-or{page}\"\n",
    "        response = requests.get(url)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Failed to fetch TripAdvisor page {page//10 + 1}\")\n",
    "            continue\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        review_blocks = soup.find_all('div', class_='review-container')\n",
    "        for block in review_blocks:\n",
    "            try:\n",
    "                reviewer = block.find('div', class_='member_info').find('div', class_='info_text').text.strip() if block.find('div', class_='member_info') else 'Anonymous'\n",
    "                date = block.find('span', class_='ratingDate')['title'] if block.find('span', class_='ratingDate') else 'N/A'\n",
    "                rating_class = block.find('span', class_='ui_bubble_rating')['class'][1] if block.find('span', class_='ui_bubble_rating') else ''\n",
    "                rating = str(int(rating_class.replace('bubble_', '')) / 10) if rating_class else 'N/A'\n",
    "                title = block.find('span', class_='noQuotes').text.strip() if block.find('span', class_='noQuotes') else 'No Title'\n",
    "                text = block.find('p', class_='partial_entry').text.strip() if block.find('p', class_='partial_entry') else 'No Text'\n",
    "                reviews.append({\n",
    "                    'source': 'TripAdvisor',\n",
    "                    'reviewer': reviewer,\n",
    "                    'date': date,\n",
    "                    'rating': rating,\n",
    "                    'title': title,\n",
    "                    'text': text\n",
    "                })\n",
    "            except Exception as e:\n",
    "                print(f\"Error parsing TripAdvisor review: {e}\")\n",
    "    return reviews\n",
    "\n",
    "# Function to save reviews to CSV\n",
    "def save_to_csv(reviews, filename='hotel_reviews.csv'):\n",
    "    if not reviews:\n",
    "        print(\"No reviews to save.\")\n",
    "        return\n",
    "    keys = reviews[0].keys()\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as output_file:\n",
    "        dict_writer = csv.DictWriter(output_file, keys)\n",
    "        dict_writer.writeheader()\n",
    "        dict_writer.writerows(reviews)\n",
    "    print(f\"Saved {len(reviews)} reviews to {filename}\")\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # URLs from provided list (use English Booking.com for simplicity; adjust if needed)\n",
    "    booking_url = \"https://www.booking.com/reviews/tn/hotel/business.en-gb.html?label=gen173nr-1FCAEoggI46AdIM1gEaEaIAQGYAQe4AQfIAQzYAQHoAQH4AQKIAgGoAgO4ApvZoMoGwAIB0gIkY2Q5ZGY5ZjMtZWEyNi00NzE5LWI5NjgtMzY4N2E4N2U3M2Q32AIG4AIB&sid=32b2429f18b4624fc09d32f51f4441cc&customer_type=total&hp_nav=0&keep_landing=1&order=featuredreviews&rows=75\"\n",
    "    tripadvisor_url = \"https://www.tripadvisor.com/Hotel_Review-g293758-d8767447-Reviews-Business_Hotel_Tunis-Tunis_Tunis_Governorate.html\"\n",
    "    \n",
    "    booking_reviews = scrape_booking(booking_url)\n",
    "    tripadvisor_reviews = scrape_tripadvisor(tripadvisor_url)\n",
    "    \n",
    "    all_reviews = booking_reviews + tripadvisor_reviews\n",
    "    save_to_csv(all_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "757201b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to fetch Momondo page 1\n",
      "Failed to fetch Momondo page 2\n",
      "Failed to process TripAdvisor page: Message: no such element: Unable to locate element: {\"method\":\"css selector\",\"selector\":\"[data-next-page]\"}\n",
      "  (Session info: chrome=142.0.7444.176); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#nosuchelementexception\n",
      "Stacktrace:\n",
      "Symbols not available. Dumping unresolved backtrace:\n",
      "\t0xd04103\n",
      "\t0xd04144\n",
      "\t0xb0e71d\n",
      "\t0xb5a03d\n",
      "\t0xb5a41b\n",
      "\t0xba17f2\n",
      "\t0xb7c954\n",
      "\t0xb9ee17\n",
      "\t0xb7c706\n",
      "\t0xb4da30\n",
      "\t0xb4ed54\n",
      "\t0xf757b4\n",
      "\t0xf7098a\n",
      "\t0xd2c392\n",
      "\t0xd1c4c8\n",
      "\t0xd2324d\n",
      "\t0xd0c478\n",
      "\t0xd0c63c\n",
      "\t0xcf67ca\n",
      "\t0x756e7ba9\n",
      "\t0x7719c3ab\n",
      "\t0x7719c32f\n",
      "\n",
      "Saved 8 reviews to hotel_reviews.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time\n",
    "\n",
    "# Headers to mimic browser\n",
    "HEADERS = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}\n",
    "\n",
    "# Function for BeautifulSoup scraping (static sites)\n",
    "def scrape_with_bs(url, source, review_selector, parse_func, pages=2):\n",
    "    reviews = []\n",
    "    for page in range(1, pages + 1):\n",
    "        paginated_url = f\"{url}&page={page}\" if 'booking' in url.lower() else url  # Adjust for site\n",
    "        response = requests.get(paginated_url, headers=HEADERS)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Failed to fetch {source} page {page}\")\n",
    "            continue\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        review_blocks = soup.select(review_selector)\n",
    "        for block in review_blocks:\n",
    "            try:\n",
    "                review = parse_func(block)\n",
    "                review['source'] = source\n",
    "                reviews.append(review)\n",
    "            except Exception as e:\n",
    "                print(f\"Error parsing {source} review: {e}\")\n",
    "    return reviews\n",
    "\n",
    "# Parse function for Booking.com\n",
    "def parse_booking(block):\n",
    "    return {\n",
    "        'reviewer': block.find('span', class_='bui-avatar-block__title').text.strip() if block.find('span', class_='bui-avatar-block__title') else 'Anonymous',\n",
    "        'date': block.find('p', class_='review_item_date').text.strip() if block.find('p', class_='review_item_date') else 'N/A',\n",
    "        'rating': block.find('div', class_='bui-review-score__badge').text.strip() if block.find('div', class_='bui-review-score__badge') else 'N/A',\n",
    "        'title': block.find('h3', class_='c-review-block__title').text.strip() if block.find('h3', class_='c-review-block__title') else 'No Title',\n",
    "        'text': ' '.join([elem.text.strip() for elem in block.find_all('span', class_='c-review__body')]) if block.find_all('span', class_='c-review__body') else 'No Text'\n",
    "    }\n",
    "\n",
    "# Parse function for Momondo (aggregated pros/cons)\n",
    "def parse_momondo(block):\n",
    "    # Momondo has sections for pros/cons; adapt to extract as 'reviews'\n",
    "    title = block.find('h3').text.strip() if block.find('h3') else 'No Title'  # e.g., 'Pros' or 'Cons'\n",
    "    items = [li.text.strip() for li in block.find_all('li')]\n",
    "    return {\n",
    "        'reviewer': 'Aggregated',\n",
    "        'date': 'N/A',\n",
    "        'rating': 'N/A',\n",
    "        'title': title,\n",
    "        'text': '; '.join(items)\n",
    "    }\n",
    "\n",
    "# Function for Selenium scraping (dynamic sites)\n",
    "def scrape_with_selenium(url, source, review_locator, parse_func, pages=2):\n",
    "    reviews = []\n",
    "    options = Options()\n",
    "    options.headless = True  # Run without GUI\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "    driver.get(url)\n",
    "    time.sleep(3)  # Wait for load\n",
    "    \n",
    "    for _ in range(pages):\n",
    "        try:\n",
    "            review_blocks = driver.find_elements(*review_locator)\n",
    "            for block in review_blocks:\n",
    "                try:\n",
    "                    review = parse_func(block)\n",
    "                    review['source'] = source\n",
    "                    reviews.append(review)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error parsing {source} review: {e}\")\n",
    "            # Click next page if exists (adapt selector per site)\n",
    "            next_button = driver.find_element(By.CSS_SELECTOR, '[data-next-page]') if source == 'TripAdvisor' else None  # Example; customize\n",
    "            if next_button:\n",
    "                next_button.click()\n",
    "                time.sleep(3)\n",
    "            else:\n",
    "                break\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to process {source} page: {e}\")\n",
    "            break\n",
    "    driver.quit()\n",
    "    return reviews\n",
    "\n",
    "# Parse function for TripAdvisor (Selenium)\n",
    "def parse_tripadvisor(block):\n",
    "    return {\n",
    "        'reviewer': block.find_element(By.CSS_SELECTOR, '.info_text').text.strip() if block.find_elements(By.CSS_SELECTOR, '.info_text') else 'Anonymous',\n",
    "        'date': block.get_attribute('title') if block.find_elements(By.CSS_SELECTOR, '.ratingDate') else 'N/A',\n",
    "        'rating': str(int(block.get_attribute('class').split('_')[1]) / 10) if block.find_elements(By.CSS_SELECTOR, '.ui_bubble_rating') else 'N/A',\n",
    "        'title': block.find_element(By.CSS_SELECTOR, '.noQuotes').text.strip() if block.find_elements(By.CSS_SELECTOR, '.noQuotes') else 'No Title',\n",
    "        'text': block.find_element(By.CSS_SELECTOR, '.partial_entry').text.strip() if block.find_elements(By.CSS_SELECTOR, '.partial_entry') else 'No Text'\n",
    "    }\n",
    "\n",
    "# Parse function for Google Maps (Selenium)\n",
    "def parse_google(block):\n",
    "    return {\n",
    "        'reviewer': block.find_element(By.CSS_SELECTOR, '.d4r55').text.strip() if block.find_elements(By.CSS_SELECTOR, '.d4r55') else 'Anonymous',  # Adapt based on inspection\n",
    "        'date': block.find_element(By.CSS_SELECTOR, '.rsqaWe').text.strip() if block.find_elements(By.CSS_SELECTOR, '.rsqaWe') else 'N/A',\n",
    "        'rating': block.get_attribute('aria-label') if block.find_elements(By.CSS_SELECTOR, '.kvMYJc') else 'N/A',  # Star rating\n",
    "        'title': 'No Title',  # Google often lacks titles\n",
    "        'text': block.find_element(By.CSS_SELECTOR, '.wiI7pd').text.strip() if block.find_elements(By.CSS_SELECTOR, '.wiI7pd') else 'No Text'\n",
    "    }\n",
    "\n",
    "# Save to CSV\n",
    "def save_to_csv(reviews, filename='hotel_reviews.csv'):\n",
    "    if not reviews:\n",
    "        print(\"No reviews to save.\")\n",
    "        return\n",
    "    keys = reviews[0].keys()\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.DictWriter(f, keys)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(reviews)\n",
    "    print(f\"Saved {len(reviews)} reviews to {filename}\")\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # URLs (use your provided ones; English Booking for simplicity)\n",
    "    booking_url = \"https://www.booking.com/reviews/tn/hotel/business.en-gb.html?label=gen173nr-1FCAEoggI46AdIM1gEaEaIAQGYAQe4AQfIAQzYAQHoAQH4AQKIAgGoAgO4ApvZoMoGwAIB0gIkY2Q5ZGY5ZjMtZWEyNi00NzE5LWI5NjgtMzY4N2E4N2U3M2Q32AIG4AIB&sid=32b2429f18b4624fc09d32f51f4441cc&customer_type=total&hp_nav=0&keep_landing=1&order=featuredreviews&rows=75\"\n",
    "    momondo_url = \"https://www.momondo.com/hotels/tunis-tunis-governorate/Business-Hotel-Tunis.mhd2417013.ksp\"\n",
    "    tripadvisor_url = \"https://www.tripadvisor.com/Hotel_Review-g293758-d8767447-Reviews-Business_Hotel_Tunis-Tunis_Tunis_Governorate.html\"\n",
    "    google_url = \"https://www.google.com/maps/place/Business+H%C3%B4tel/@36.818325,10.1820211,17z/data=!4m8!3m7!1s0x12fd3489b5a4f4e1:0xdda07f445b5b03cd!8m2!3d36.818325!4d10.184596!9m1!1b1!16s%2Fg%2F11bwn563xh?entry=ttu\"\n",
    "    \n",
    "    # Scrape static sites\n",
    "    booking_reviews = scrape_with_bs(booking_url, 'Booking.com', 'div.review_list_new_item_block', parse_booking)\n",
    "    momondo_reviews = scrape_with_bs(momondo_url, 'Momondo', 'div#reviews ul', parse_momondo)  # Adjust selector to target pros/cons lists\n",
    "    \n",
    "    # Scrape dynamic sites\n",
    "    tripadvisor_reviews = scrape_with_selenium(tripadvisor_url, 'TripAdvisor', (By.CLASS_NAME, 'review-container'), parse_tripadvisor)\n",
    "    google_reviews = scrape_with_selenium(google_url, 'Google Maps', (By.CLASS_NAME, 'jftiEf'), parse_google)  # Google review class; inspect for accuracy\n",
    "    \n",
    "    all_reviews = booking_reviews + momondo_reviews + tripadvisor_reviews + google_reviews\n",
    "    save_to_csv(all_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "823d29db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Scraping Booking.com ---\n",
      "Blocked or error on page 1\n",
      "--- Scraping TripAdvisor ---\n",
      "TripAdvisor: Page 1 processed.\n",
      "TripAdvisor: No next button found or reached end.\n",
      "--- Scraping Google Maps ---\n",
      "Google Maps: Could not find scrollable feed. Check URL or Selectors.\n",
      "No reviews extracted.\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "import time\n",
    "import random\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# --- CONFIGURATION ---\n",
    "HEADERS = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',\n",
    "    'Accept-Language': 'en-US,en;q=0.9'\n",
    "}\n",
    "\n",
    "# --- HELPER: SETUP DRIVER ---\n",
    "def get_driver():\n",
    "    options = Options()\n",
    "    # options.add_argument(\"--headless=new\")  # Uncomment to run invisible. Run visible first to debug!\n",
    "    options.add_argument(\"--window-size=1920,1080\")\n",
    "    options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "    options.add_argument(f\"user-agent={HEADERS['User-Agent']}\")\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "    return driver\n",
    "\n",
    "# --- 1. STATIC SCRAPING (Booking.com) ---\n",
    "def scrape_booking(url, pages=1):\n",
    "    print(\"--- Scraping Booking.com ---\")\n",
    "    reviews = []\n",
    "    for page in range(pages):\n",
    "        # Booking pagination is largely offset based usually\n",
    "        offset = page * 25\n",
    "        # Ensure URL handles pagination correctly\n",
    "        target_url = f\"{url};rows=25;offset={offset}\"\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(target_url, headers=HEADERS, timeout=10)\n",
    "            if response.status_code != 200:\n",
    "                print(f\"Blocked or error on page {page+1}\")\n",
    "                break\n",
    "            \n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            # Generic selector for review cards on Booking (changes often, check 'li.review_list_new_item_block')\n",
    "            review_blocks = soup.select('li.review_list_new_item_block')\n",
    "            \n",
    "            if not review_blocks:\n",
    "                print(\"No review blocks found (Selectors might need update via Inspect Element).\")\n",
    "                break\n",
    "\n",
    "            for block in review_blocks:\n",
    "                reviews.append({\n",
    "                    'source': 'Booking.com',\n",
    "                    'reviewer': block.find('span', class_='bui-avatar-block__title').text.strip() if block.find('span', class_='bui-avatar-block__title') else 'Anonymous',\n",
    "                    'rating': block.find('div', class_='bui-review-score__badge').text.strip() if block.find('div', class_='bui-review-score__badge') else 'N/A',\n",
    "                    'text': block.find('span', class_='c-review__body').text.strip() if block.find('span', class_='c-review__body') else 'No Text'\n",
    "                })\n",
    "            print(f\"Booking: Collected {len(review_blocks)} reviews from page {page+1}\")\n",
    "            time.sleep(random.uniform(2, 5)) # Polite delay\n",
    "        except Exception as e:\n",
    "            print(f\"Error on Booking: {e}\")\n",
    "            \n",
    "    return reviews\n",
    "\n",
    "# --- 2. DYNAMIC SCRAPING (TripAdvisor - Pagination) ---\n",
    "def scrape_tripadvisor(url, pages=1):\n",
    "    print(\"--- Scraping TripAdvisor ---\")\n",
    "    driver = get_driver()\n",
    "    reviews = []\n",
    "    \n",
    "    try:\n",
    "        driver.get(url)\n",
    "        time.sleep(3)\n",
    "        \n",
    "        for i in range(pages):\n",
    "            # 1. Expand reviews if \"Read more\" exists\n",
    "            try:\n",
    "                more_btns = driver.find_elements(By.CSS_SELECTOR, \"span[data-test-target='review-text'] span.ui_icon\")\n",
    "                for btn in more_btns:\n",
    "                    driver.execute_script(\"arguments[0].click();\", btn)\n",
    "                time.sleep(1)\n",
    "            except: pass\n",
    "\n",
    "            # 2. Extract Data (Selectors updated for 2024/2025 layouts)\n",
    "            # TripAdvisor classes are messy (e.g., 'yCeTE', 'yFGwA'). We try to target data-test-target.\n",
    "            blocks = driver.find_elements(By.CSS_SELECTOR, \"div[data-test-target='reviews-tab'] > div\")\n",
    "            \n",
    "            # Fallback if specific container not found\n",
    "            if not blocks:\n",
    "                blocks = driver.find_elements(By.CLASS_NAME, \"review-container\")\n",
    "\n",
    "            for block in blocks:\n",
    "                try:\n",
    "                    text_elem = block.find_element(By.CSS_SELECTOR, \"span[class*='ui_bubble_rating']\")\n",
    "                    rating = text_elem.get_attribute(\"class\").split(\"_\")[-1] # e.g., \"50\" -> 5.0\n",
    "                    \n",
    "                    reviews.append({\n",
    "                        'source': 'TripAdvisor',\n",
    "                        'reviewer': 'Traveler',\n",
    "                        'rating': f\"{int(rating)/10}\",\n",
    "                        'text': block.text.replace(\"\\n\", \" \")[:200] + \"...\" # Truncated for safety\n",
    "                    })\n",
    "                except: continue\n",
    "            \n",
    "            print(f\"TripAdvisor: Page {i+1} processed.\")\n",
    "\n",
    "            # 3. Next Page\n",
    "            try:\n",
    "                # Look for the next button specifically\n",
    "                next_btn = driver.find_element(By.XPATH, \"//a[contains(text(), 'Next')] | //a[contains(@aria-label, 'Next page')]\")\n",
    "                if \"disabled\" in next_btn.get_attribute(\"class\"):\n",
    "                    break\n",
    "                next_btn.click()\n",
    "                time.sleep(random.uniform(3, 6))\n",
    "            except Exception:\n",
    "                print(\"TripAdvisor: No next button found or reached end.\")\n",
    "                break\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"TripAdvisor Error: {e}\")\n",
    "    finally:\n",
    "        driver.quit()\n",
    "    \n",
    "    return reviews\n",
    "\n",
    "# --- 3. DYNAMIC SCRAPING (Google Maps - Infinite Scroll) ---\n",
    "def scrape_google_maps(url, max_reviews=20):\n",
    "    print(\"--- Scraping Google Maps ---\")\n",
    "    driver = get_driver()\n",
    "    reviews = []\n",
    "    \n",
    "    try:\n",
    "        driver.get(url)\n",
    "        # Wait for the sidebar to load\n",
    "        wait = WebDriverWait(driver, 10)\n",
    "        # Try to find the scrollable container (the sidebar)\n",
    "        # Note: Class names like 'm6QErb' are obfuscated and change. \n",
    "        # We try to find the container with aria-label=\"Reviews\" or similar.\n",
    "        time.sleep(3)\n",
    "        \n",
    "        # Simple scroll logic for the sidebar\n",
    "        scrollable_div = None\n",
    "        try:\n",
    "            # Common selector for the scrollable review list in Google Maps\n",
    "            scrollable_div = driver.find_element(By.CSS_SELECTOR, 'div[role=\"feed\"]')\n",
    "        except:\n",
    "            print(\"Google Maps: Could not find scrollable feed. Check URL or Selectors.\")\n",
    "            return []\n",
    "\n",
    "        last_height = driver.execute_script(\"return arguments[0].scrollHeight\", scrollable_div)\n",
    "        \n",
    "        while len(reviews) < max_reviews:\n",
    "            # Parse visible reviews\n",
    "            blocks = driver.find_elements(By.CSS_SELECTOR, 'div.jftiEf')\n",
    "            \n",
    "            for block in blocks[len(reviews):]: # Only process new ones\n",
    "                try:\n",
    "                    text = block.find_element(By.CSS_SELECTOR, 'span.wiI7pd').text\n",
    "                    rating = block.find_element(By.CSS_SELECTOR, 'span.kvMYJc').get_attribute(\"aria-label\")\n",
    "                    reviews.append({\n",
    "                        'source': 'Google Maps',\n",
    "                        'reviewer': 'Google User',\n",
    "                        'rating': rating,\n",
    "                        'text': text\n",
    "                    })\n",
    "                except: pass\n",
    "                \n",
    "            if len(reviews) >= max_reviews: break\n",
    "\n",
    "            # Scroll down\n",
    "            driver.execute_script(\"arguments[0].scrollTo(0, arguments[0].scrollHeight)\", scrollable_div)\n",
    "            time.sleep(2)\n",
    "            \n",
    "            new_height = driver.execute_script(\"return arguments[0].scrollHeight\", scrollable_div)\n",
    "            if new_height == last_height:\n",
    "                break # End of list\n",
    "            last_height = new_height\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Google Maps Error: {e}\")\n",
    "    finally:\n",
    "        driver.quit()\n",
    "        \n",
    "    return reviews\n",
    "\n",
    "# --- MAIN ---\n",
    "if __name__ == \"__main__\":\n",
    "    # VALID URLS (Example: Hotel Africa Tunis)\n",
    "    booking_url = \"https://www.booking.com/hotel/tn/africa-tunis.en-gb.html\"\n",
    "    ta_url = \"https://www.tripadvisor.com/Hotel_Review-g293758-d302476-Reviews-El_Mouradi_Hotel_Africa_Tunis-Tunis_Tunis_Governorate.html\"\n",
    "    # Note: Google Maps URLs must be the full map view URL, not the redirect\n",
    "    google_url = \"https://www.google.com/maps/place/El+Mouradi+Hotel+Africa+Tunis/@36.8000678,10.180014,17z/data=!4m12!3m11!1s0x12fd3464506041c9:0x5e0f06f50b4d0810!5m2!4m1!1i2!8m2!3d36.8000678!4d10.1825889!9m1!1b1!16s%2Fg%2F11b6_y_y7_?entry=ttu\"\n",
    "\n",
    "    all_data = []\n",
    "    \n",
    "    # 1. Booking\n",
    "    all_data.extend(scrape_booking(booking_url, pages=1))\n",
    "    \n",
    "    # 2. TripAdvisor\n",
    "    all_data.extend(scrape_tripadvisor(ta_url, pages=1))\n",
    "    \n",
    "    # 3. Google Maps\n",
    "    all_data.extend(scrape_google_maps(google_url, max_reviews=10))\n",
    "\n",
    "    # Save\n",
    "    if all_data:\n",
    "        keys = all_data[0].keys()\n",
    "        with open('final_reviews.csv', 'w', newline='', encoding='utf-8') as f:\n",
    "            writer = csv.DictWriter(f, keys)\n",
    "            writer.writeheader()\n",
    "            writer.writerows(all_data)\n",
    "        print(f\"Successfully saved {len(all_data)} reviews to final_reviews.csv\")\n",
    "    else:\n",
    "        print(\"No reviews extracted.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d593003f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to fetch Momondo page 1\n",
      "Failed to fetch Momondo page 2\n",
      "No next button on TripAdvisor page 1\n",
      "Saved 28 reviews to hotel_reviews.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time\n",
    "\n",
    "# Headers to mimic browser\n",
    "HEADERS = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}\n",
    "\n",
    "# Function for BeautifulSoup scraping (static sites)\n",
    "def scrape_with_bs(url, source, review_selector, parse_func, pages=2):\n",
    "    reviews = []\n",
    "    for page in range(1, pages + 1):\n",
    "        paginated_url = f\"{url}&page={page}\" if 'booking' in url.lower() else url  # Adjust for site\n",
    "        response = requests.get(paginated_url, headers=HEADERS)\n",
    "        if response.status_code != 200:\n",
    "            print(f\"Failed to fetch {source} page {page}\")\n",
    "            continue\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        review_blocks = soup.select(review_selector)\n",
    "        for block in review_blocks:\n",
    "            try:\n",
    "                review = parse_func(soup if source == 'Momondo' else block)  # Pass soup for Momondo as it's aggregated\n",
    "                review['source'] = source\n",
    "                reviews.append(review)\n",
    "            except Exception as e:\n",
    "                print(f\"Error parsing {source} review: {e}\")\n",
    "    return reviews\n",
    "\n",
    "# Parse function for Booking.com (unchanged)\n",
    "def parse_booking(block):\n",
    "    return {\n",
    "        'reviewer': block.find('span', class_='bui-avatar-block__title').text.strip() if block.find('span', class_='bui-avatar-block__title') else 'Anonymous',\n",
    "        'date': block.find('p', class_='review_item_date').text.strip() if block.find('p', class_='review_item_date') else 'N/A',\n",
    "        'rating': block.find('div', class_='bui-review-score__badge').text.strip() if block.find('div', class_='bui-review-score__badge') else 'N/A',\n",
    "        'title': block.find('h3', class_='c-review-block__title').text.strip() if block.find('h3', class_='c-review-block__title') else 'No Title',\n",
    "        'text': ' '.join([elem.text.strip() for elem in block.find_all('span', class_='c-review__body')]) if block.find_all('span', class_='c-review__body') else 'No Text'\n",
    "    }\n",
    "\n",
    "# Parse function for Momondo (adjusted for aggregated pros/cons using string search)\n",
    "def parse_momondo(soup):\n",
    "    reviews = []\n",
    "    pros_heading = soup.find(string=lambda text: text and 'Pros +' in text)\n",
    "    if pros_heading:\n",
    "        pros_list = pros_heading.find_next('ul')\n",
    "        if pros_list:\n",
    "            items = [li.text.strip() for li in pros_list.find_all('li')]\n",
    "            reviews.append({\n",
    "                'reviewer': 'Aggregated',\n",
    "                'date': 'N/A',\n",
    "                'rating': 'N/A',\n",
    "                'title': 'Pros',\n",
    "                'text': '; '.join(items)\n",
    "            })\n",
    "\n",
    "    cons_heading = soup.find(string=lambda text: text and 'Cons -' in text)\n",
    "    if cons_heading:\n",
    "        cons_list = cons_heading.find_next('ul')\n",
    "        if cons_list:\n",
    "            items = [li.text.strip() for li in cons_list.find_all('li')]\n",
    "            reviews.append({\n",
    "                'reviewer': 'Aggregated',\n",
    "                'date': 'N/A',\n",
    "                'rating': 'N/A',\n",
    "                'title': 'Cons',\n",
    "                'text': '; '.join(items)\n",
    "            })\n",
    "    return reviews[0] if reviews else {'reviewer': 'N/A', 'date': 'N/A', 'rating': 'N/A', 'title': 'No Data', 'text': 'No pros/cons found'}\n",
    "\n",
    "# Function for Selenium scraping (dynamic sites)\n",
    "def scrape_with_selenium(url, source, review_locator, parse_func, pages=2):\n",
    "    reviews = []\n",
    "    options = Options()\n",
    "    options.headless = True\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "    driver.get(url)\n",
    "    time.sleep(3)  # Initial load\n",
    "\n",
    "    for page_num in range(pages):\n",
    "        if source == 'Google Maps':\n",
    "            # Scroll reviews panel to load more\n",
    "            try:\n",
    "                reviews_panel = driver.find_element(By.CSS_SELECTOR, '.m6QErb.DxyBCb.kA9KIf.dS8AEf.XiKgde')\n",
    "                for _ in range(5):  # Scroll multiple times to load\n",
    "                    driver.execute_script(\"arguments[0].scrollBy(0, 2000);\", reviews_panel)\n",
    "                    time.sleep(1)\n",
    "            except:\n",
    "                print(\"No reviews panel found on Google Maps\")\n",
    "\n",
    "        try:\n",
    "            review_blocks = driver.find_elements(*review_locator)\n",
    "            for block in review_blocks:\n",
    "                try:\n",
    "                    review = parse_func(block)\n",
    "                    review['source'] = source\n",
    "                    reviews.append(review)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error parsing {source} review: {e}\")\n",
    "            # Pagination\n",
    "            if source == 'TripAdvisor':\n",
    "                try:\n",
    "                    next_button = driver.find_element(By.CSS_SELECTOR, '.ui_button.nav.next.primary')\n",
    "                    if next_button.is_enabled():\n",
    "                        next_button.click()\n",
    "                        time.sleep(3)\n",
    "                    else:\n",
    "                        break\n",
    "                except:\n",
    "                    print(f\"No next button on TripAdvisor page {page_num + 1}\")\n",
    "                    break\n",
    "            else:\n",
    "                break  # No pagination for Google in this setup\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to process {source} page {page_num + 1}: {e}\")\n",
    "            break\n",
    "    driver.quit()\n",
    "    return reviews\n",
    "\n",
    "# Parse function for TripAdvisor (unchanged)\n",
    "def parse_tripadvisor(block):\n",
    "    return {\n",
    "        'reviewer': block.find_element(By.CSS_SELECTOR, '.info_text').text.strip() if block.find_elements(By.CSS_SELECTOR, '.info_text') else 'Anonymous',\n",
    "        'date': block.get_attribute('title') if block.find_elements(By.CSS_SELECTOR, '.ratingDate') else 'N/A',\n",
    "        'rating': str(int(block.get_attribute('class').split('_')[1]) / 10) if block.find_elements(By.CSS_SELECTOR, '.ui_bubble_rating') else 'N/A',\n",
    "        'title': block.find_element(By.CSS_SELECTOR, '.noQuotes').text.strip() if block.find_elements(By.CSS_SELECTOR, '.noQuotes') else 'No Title',\n",
    "        'text': block.find_element(By.CSS_SELECTOR, '.partial_entry').text.strip() if block.find_elements(By.CSS_SELECTOR, '.partial_entry') else 'No Text'\n",
    "    }\n",
    "\n",
    "# Parse function for Google Maps (unchanged, standard selectors)\n",
    "def parse_google(block):\n",
    "    return {\n",
    "        'reviewer': block.find_element(By.CSS_SELECTOR, '.d4r55').text.strip() if block.find_elements(By.CSS_SELECTOR, '.d4r55') else 'Anonymous',\n",
    "        'date': block.find_element(By.CSS_SELECTOR, '.rsqaWe').text.strip() if block.find_elements(By.CSS_SELECTOR, '.rsqaWe') else 'N/A',\n",
    "        'rating': block.get_attribute('aria-label') if block.find_elements(By.CSS_SELECTOR, '.kvMYJc') else 'N/A',\n",
    "        'title': 'No Title',\n",
    "        'text': block.find_element(By.CSS_SELECTOR, '.wiI7pd').text.strip() if block.find_elements(By.CSS_SELECTOR, '.wiI7pd') else 'No Text'\n",
    "    }\n",
    "\n",
    "# Save to CSV\n",
    "def save_to_csv(reviews, filename='hotel_reviews_3.csv'):\n",
    "    if not reviews:\n",
    "        print(\"No reviews to save.\")\n",
    "        return\n",
    "    keys = reviews[0].keys()\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.DictWriter(f, keys)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(reviews)\n",
    "    print(f\"Saved {len(reviews)} reviews to {filename}\")\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # URLs\n",
    "    booking_url = \"https://www.booking.com/reviews/tn/hotel/business.en-gb.html?label=gen173nr-1FCAEoggI46AdIM1gEaEaIAQGYAQe4AQfIAQzYAQHoAQH4AQKIAgGoAgO4ApvZoMoGwAIB0gIkY2Q5ZGY5ZjMtZWEyNi00NzE5LWI5NjgtMzY4N2E4N2U3M2Q32AIG4AIB&sid=32b2429f18b4624fc09d32f51f4441cc&customer_type=total&hp_nav=0&keep_landing=1&order=featuredreviews&rows=75\"\n",
    "    momondo_url = \"https://www.momondo.com/hotels/tunis-tunis-governorate/Business-Hotel-Tunis.mhd2417013.ksp\"\n",
    "    tripadvisor_url = \"https://www.tripadvisor.com/Hotel_Review-g293758-d8767447-Reviews-Business_Hotel_Tunis-Tunis_Tunis_Governorate.html\"\n",
    "    google_url = \"https://www.google.com/maps/place/Business+H%C3%B4tel/@36.818325,10.1820211,17z/data=!4m8!3m7!1s0x12fd3489b5a4f4e1:0xdda07f445b5b03cd!8m2!3d36.818325!4d10.184596!9m1!1b1!16s%2Fg%2F11bwn563xh?entry=ttu\"\n",
    "    \n",
    "    # Scrape static sites\n",
    "    booking_reviews = scrape_with_bs(booking_url, 'Booking.com', 'div.review_list_new_item_block', parse_booking)\n",
    "    momondo_reviews = scrape_with_bs(momondo_url, 'Momondo', '', parse_momondo)  # Empty selector as we pass soup\n",
    "    \n",
    "    # Scrape dynamic sites\n",
    "    tripadvisor_reviews = scrape_with_selenium(tripadvisor_url, 'TripAdvisor', (By.CLASS_NAME, 'review-container'), parse_tripadvisor)\n",
    "    google_reviews = scrape_with_selenium(google_url, 'Google Maps', (By.CLASS_NAME, 'jftiEf'), parse_google)\n",
    "    \n",
    "    all_reviews = booking_reviews + momondo_reviews + tripadvisor_reviews + google_reviews\n",
    "    save_to_csv(all_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5989e16e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Booking.com fetch status: 200\n",
      "Booking.com fetch status: 200\n",
      "Momondo fetch status: 406\n",
      "Failed to fetch Momondo page 1 (status: 406)\n",
      "No reviews found on TripAdvisor page 1\n",
      "No next button found on TripAdvisor page 1\n",
      "Saved 58 reviews to hotel_reviews___.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time\n",
    "\n",
    "# Headers to mimic browser and handle potential redirects\n",
    "HEADERS = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "    'Accept-Language': 'en-US,en;q=0.9'\n",
    "}\n",
    "\n",
    "# Function for BeautifulSoup scraping (static sites)\n",
    "def scrape_with_bs(url, source, review_selector, parse_func, pages=2):\n",
    "    reviews = []\n",
    "    if source == 'Momondo':\n",
    "        pages = 1  # No pagination for Momondo\n",
    "    for page in range(1, pages + 1):\n",
    "        paginated_url = f\"{url}&page={page}\" if 'booking' in url.lower() else url\n",
    "        response = requests.get(paginated_url, headers=HEADERS, allow_redirects=True)\n",
    "        print(f\"{source} fetch status: {response.status_code}\")  # Log status for debugging\n",
    "        if response.status_code not in [200, 301, 302]:\n",
    "            print(f\"Failed to fetch {source} page {page} (status: {response.status_code})\")\n",
    "            continue\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        if source == 'Momondo':\n",
    "            # For Momondo, parse entire soup directly\n",
    "            parsed_reviews = parse_func(soup)\n",
    "            for pr in parsed_reviews:\n",
    "                pr['source'] = source\n",
    "                reviews.append(pr)\n",
    "        else:\n",
    "            review_blocks = soup.select(review_selector)\n",
    "            for block in review_blocks:\n",
    "                try:\n",
    "                    review = parse_func(block)\n",
    "                    review['source'] = source\n",
    "                    reviews.append(review)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error parsing {source} review: {e}\")\n",
    "    return reviews\n",
    "\n",
    "# Parse function for Booking.com (unchanged)\n",
    "def parse_booking(block):\n",
    "    return {\n",
    "        'reviewer': block.find('span', class_='bui-avatar-block__title').text.strip() if block.find('span', class_='bui-avatar-block__title') else 'Anonymous',\n",
    "        'date': block.find('p', class_='review_item_date').text.strip() if block.find('p', class_='review_item_date') else 'N/A',\n",
    "        'rating': block.find('div', class_='bui-review-score__badge').text.strip() if block.find('div', class_='bui-review-score__badge') else 'N/A',\n",
    "        'title': block.find('h3', class_='c-review-block__title').text.strip() if block.find('h3', class_='c-review-block__title') else 'No Title',\n",
    "        'text': ' '.join([elem.text.strip() for elem in block.find_all('span', class_='c-review__body')]) if block.find_all('span', class_='c-review__body') else 'No Text'\n",
    "    }\n",
    "\n",
    "# Parse function for Momondo (updated with precise selectors)\n",
    "def parse_momondo(soup):\n",
    "    reviews = []\n",
    "    pros_h3 = soup.find('h3', string=lambda text: text and 'Pros +' in text.strip())\n",
    "    if pros_h3:\n",
    "        pros_ul = pros_h3.find_next('ul', class_='pros-list')\n",
    "        if pros_ul:\n",
    "            items = [li.text.strip() for li in pros_ul.find_all('li')]\n",
    "            reviews.append({\n",
    "                'reviewer': 'Aggregated',\n",
    "                'date': 'N/A',\n",
    "                'rating': 'N/A',\n",
    "                'title': 'Pros',\n",
    "                'text': '; '.join(items)\n",
    "            })\n",
    "\n",
    "    cons_h3 = soup.find('h3', string=lambda text: text and 'Cons -' in text.strip())\n",
    "    if cons_h3:\n",
    "        cons_ul = cons_h3.find_next('ul', class_='cons-list')\n",
    "        if cons_ul:\n",
    "            items = [li.text.strip() for li in cons_ul.find_all('li')]\n",
    "            reviews.append({\n",
    "                'reviewer': 'Aggregated',\n",
    "                'date': 'N/A',\n",
    "                'rating': 'N/A',\n",
    "                'title': 'Cons',\n",
    "                'text': '; '.join(items)\n",
    "            })\n",
    "    return reviews if reviews else [{'reviewer': 'N/A', 'date': 'N/A', 'rating': 'N/A', 'title': 'No Data', 'text': 'No pros/cons found'}]\n",
    "\n",
    "# Function for Selenium scraping (dynamic sites)\n",
    "def scrape_with_selenium(url, source, review_locator, parse_func, pages=2):\n",
    "    reviews = []\n",
    "    options = Options()\n",
    "    options.headless = True\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "    driver.get(url)\n",
    "    time.sleep(5)  # Longer initial load for stability\n",
    "\n",
    "    for page_num in range(pages):\n",
    "        if source == 'Google Maps':\n",
    "            try:\n",
    "                reviews_panel = driver.find_element(By.CSS_SELECTOR, '.m6QErb.DxyBCb.kA9KIf.dS8AEf.XiKgde')\n",
    "                for _ in range(10):  # Increased scrolls to load more\n",
    "                    driver.execute_script(\"arguments[0].scrollBy(0, 2000);\", reviews_panel)\n",
    "                    time.sleep(1.5)\n",
    "            except:\n",
    "                print(\"No reviews panel found on Google Maps\")\n",
    "\n",
    "        try:\n",
    "            review_blocks = driver.find_elements(*review_locator)\n",
    "            if not review_blocks:\n",
    "                print(f\"No reviews found on {source} page {page_num + 1}\")\n",
    "            for block in review_blocks:\n",
    "                try:\n",
    "                    review = parse_func(block)\n",
    "                    review['source'] = source\n",
    "                    reviews.append(review)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error parsing {source} review: {e}\")\n",
    "            # Pagination\n",
    "            if source == 'TripAdvisor':\n",
    "                try:\n",
    "                    next_button = driver.find_element(By.CSS_SELECTOR, '.ui_button.nav.next.primary')\n",
    "                    if next_button and next_button.is_displayed() and next_button.is_enabled():\n",
    "                        next_button.click()\n",
    "                        time.sleep(5)\n",
    "                    else:\n",
    "                        print(f\"No next button or not enabled on TripAdvisor page {page_num + 1}\")\n",
    "                        break\n",
    "                except:\n",
    "                    print(f\"No next button found on TripAdvisor page {page_num + 1}\")\n",
    "                    break\n",
    "            else:\n",
    "                break\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to process {source} page {page_num + 1}: {e}\")\n",
    "            break\n",
    "    driver.quit()\n",
    "    return reviews\n",
    "\n",
    "# Parse function for TripAdvisor (added robustness)\n",
    "def parse_tripadvisor(block):\n",
    "    try:\n",
    "        reviewer_elem = block.find_elements(By.CSS_SELECTOR, '.info_text div:first-child')\n",
    "        reviewer = reviewer_elem[0].text.strip() if reviewer_elem else 'Anonymous'\n",
    "        date_elem = block.find_elements(By.CSS_SELECTOR, '.ratingDate')\n",
    "        date = date_elem[0].get_attribute('title') if date_elem else 'N/A'\n",
    "        rating_elem = block.find_elements(By.CSS_SELECTOR, '.ui_bubble_rating')\n",
    "        rating_class = rating_elem[0].get_attribute('class') if rating_elem else ''\n",
    "        rating = str(int(rating_class.split('_')[-1]) / 10) if rating_class else 'N/A'\n",
    "        title_elem = block.find_elements(By.CSS_SELECTOR, '.noQuotes')\n",
    "        title = title_elem[0].text.strip() if title_elem else 'No Title'\n",
    "        text_elem = block.find_elements(By.CSS_SELECTOR, '.partial_entry')\n",
    "        text = text_elem[0].text.strip() if text_elem else 'No Text'\n",
    "        return {\n",
    "            'reviewer': reviewer,\n",
    "            'date': date,\n",
    "            'rating': rating,\n",
    "            'title': title,\n",
    "            'text': text\n",
    "        }\n",
    "    except:\n",
    "        return {'reviewer': 'N/A', 'date': 'N/A', 'rating': 'N/A', 'title': 'No Data', 'text': 'Parse failed'}\n",
    "\n",
    "# Parse function for Google Maps (unchanged)\n",
    "def parse_google(block):\n",
    "    return {\n",
    "        'reviewer': block.find_element(By.CSS_SELECTOR, '.d4r55').text.strip() if block.find_elements(By.CSS_SELECTOR, '.d4r55') else 'Anonymous',\n",
    "        'date': block.find_element(By.CSS_SELECTOR, '.rsqaWe').text.strip() if block.find_elements(By.CSS_SELECTOR, '.rsqaWe') else 'N/A',\n",
    "        'rating': block.get_attribute('aria-label') if block.find_elements(By.CSS_SELECTOR, '.kvMYJc') else 'N/A',\n",
    "        'title': 'No Title',\n",
    "        'text': block.find_element(By.CSS_SELECTOR, '.wiI7pd').text.strip() if block.find_elements(By.CSS_SELECTOR, '.wiI7pd') else 'No Text'\n",
    "    }\n",
    "\n",
    "# Save to CSV\n",
    "def save_to_csv(reviews, filename='hotel_reviews___.csv'):\n",
    "    if not reviews:\n",
    "        print(\"No reviews to save.\")\n",
    "        return\n",
    "    keys = reviews[0].keys()\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.DictWriter(f, keys)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(reviews)\n",
    "    print(f\"Saved {len(reviews)} reviews to {filename}\")\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # URLs\n",
    "    booking_url = \"https://www.booking.com/reviews/tn/hotel/business.en-gb.html?label=gen173nr-1FCAEoggI46AdIM1gEaEaIAQGYAQe4AQfIAQzYAQHoAQH4AQKIAgGoAgO4ApvZoMoGwAIB0gIkY2Q5ZGY5ZjMtZWEyNi00NzE5LWI5NjgtMzY4N2E4N2U3M2Q32AIG4AIB&sid=32b2429f18b4624fc09d32f51f4441cc&customer_type=total&hp_nav=0&keep_landing=1&order=featuredreviews&rows=75\"\n",
    "    momondo_url = \"https://www.momondo.com/hotels/tunis-tunis-governorate/Business-Hotel-Tunis.mhd2417013.ksp\"\n",
    "    tripadvisor_url = \"https://www.tripadvisor.com/Hotel_Review-g293758-d8767447-Reviews-Business_Hotel_Tunis-Tunis_Tunis_Governorate.html\"\n",
    "    google_url = \"https://www.google.com/maps/place/Business+H%C3%B4tel/@36.818325,10.1820211,17z/data=!4m8!3m7!1s0x12fd3489b5a4f4e1:0xdda07f445b5b03cd!8m2!3d36.818325!4d10.184596!9m1!1b1!16s%2Fg%2F11bwn563xh?entry=ttu\"\n",
    "    \n",
    "    # Scrape static sites\n",
    "    booking_reviews = scrape_with_bs(booking_url, 'Booking.com', 'div.review_list_new_item_block', parse_booking)\n",
    "    momondo_reviews = scrape_with_bs(momondo_url, 'Momondo', '', parse_momondo)  # Empty selector for soup parsing\n",
    "    \n",
    "    # Scrape dynamic sites\n",
    "    tripadvisor_reviews = scrape_with_selenium(tripadvisor_url, 'TripAdvisor', (By.CLASS_NAME, 'review-container'), parse_tripadvisor)\n",
    "    google_reviews = scrape_with_selenium(google_url, 'Google Maps', (By.CLASS_NAME, 'jftiEf'), parse_google)\n",
    "    \n",
    "    all_reviews = booking_reviews + momondo_reviews + tripadvisor_reviews + google_reviews\n",
    "    save_to_csv(all_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "742ff391",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Booking.com fetch status: 200\n",
      "Booking.com fetch status: 200\n",
      "Momondo fetch status: 200\n",
      "No reviews found on TripAdvisor page 1\n",
      "No next button found on TripAdvisor page 1\n",
      "Saved 59 reviews to hotel_reviews__1.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time\n",
    "\n",
    "# Enhanced headers to mimic full browser request and avoid 406\n",
    "HEADERS = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',\n",
    "    'Accept-Language': 'en-US,en;q=0.9',\n",
    "    'Accept-Encoding': 'gzip, deflate, br',\n",
    "    'Sec-Fetch-Mode': 'navigate',\n",
    "    'Sec-Fetch-Site': 'none',\n",
    "    'Sec-Fetch-User': '?1',\n",
    "    'Upgrade-Insecure-Requests': '1'\n",
    "}\n",
    "\n",
    "# Function for BeautifulSoup scraping (static sites)\n",
    "def scrape_with_bs(url, source, review_selector, parse_func, pages=2):\n",
    "    reviews = []\n",
    "    if source == 'Momondo':\n",
    "        pages = 1  # No pagination for Momondo\n",
    "    for page in range(1, pages + 1):\n",
    "        paginated_url = f\"{url}&page={page}\" if 'booking' in url.lower() else url\n",
    "        response = requests.get(paginated_url, headers=HEADERS, allow_redirects=True)\n",
    "        print(f\"{source} fetch status: {response.status_code}\")  # Log status for debugging\n",
    "        if response.status_code not in [200, 301, 302]:\n",
    "            print(f\"Failed to fetch {source} page {page} (status: {response.status_code})\")\n",
    "            continue\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        if source == 'Momondo':\n",
    "            # For Momondo, parse entire soup directly\n",
    "            parsed_reviews = parse_func(soup)\n",
    "            for pr in parsed_reviews:\n",
    "                pr['source'] = source\n",
    "                reviews.append(pr)\n",
    "        else:\n",
    "            review_blocks = soup.select(review_selector)\n",
    "            for block in review_blocks:\n",
    "                try:\n",
    "                    review = parse_func(block)\n",
    "                    review['source'] = source\n",
    "                    reviews.append(review)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error parsing {source} review: {e}\")\n",
    "    return reviews\n",
    "\n",
    "# Parse function for Booking.com (unchanged)\n",
    "def parse_booking(block):\n",
    "    return {\n",
    "        'reviewer': block.find('span', class_='bui-avatar-block__title').text.strip() if block.find('span', class_='bui-avatar-block__title') else 'Anonymous',\n",
    "        'date': block.find('p', class_='review_item_date').text.strip() if block.find('p', class_='review_item_date') else 'N/A',\n",
    "        'rating': block.find('div', class_='bui-review-score__badge').text.strip() if block.find('div', class_='bui-review-score__badge') else 'N/A',\n",
    "        'title': block.find('h3', class_='c-review-block__title').text.strip() if block.find('h3', class_='c-review-block__title') else 'No Title',\n",
    "        'text': ' '.join([elem.text.strip() for elem in block.find_all('span', class_='c-review__body')]) if block.find_all('span', class_='c-review__body') else 'No Text'\n",
    "    }\n",
    "\n",
    "# Parse function for Momondo (refined selectors without class assumption)\n",
    "def parse_momondo(soup):\n",
    "    reviews = []\n",
    "    pros_h3 = soup.find('h3', string=lambda text: text and 'Pros +' in text.strip())\n",
    "    if pros_h3:\n",
    "        pros_ul = pros_h3.find_next('ul')\n",
    "        if pros_ul:\n",
    "            items = [li.text.strip() for li in pros_ul.find_all('li')]\n",
    "            reviews.append({\n",
    "                'reviewer': 'Aggregated',\n",
    "                'date': 'N/A',\n",
    "                'rating': 'N/A',\n",
    "                'title': 'Pros',\n",
    "                'text': '; '.join(items)\n",
    "            })\n",
    "\n",
    "    cons_h3 = soup.find('h3', string=lambda text: text and 'Cons -' in text.strip())\n",
    "    if cons_h3:\n",
    "        cons_ul = cons_h3.find_next('ul')\n",
    "        if cons_ul:\n",
    "            items = [li.text.strip() for li in cons_ul.find_all('li')]\n",
    "            reviews.append({\n",
    "                'reviewer': 'Aggregated',\n",
    "                'date': 'N/A',\n",
    "                'rating': 'N/A',\n",
    "                'title': 'Cons',\n",
    "                'text': '; '.join(items)\n",
    "            })\n",
    "    return reviews if reviews else [{'reviewer': 'N/A', 'date': 'N/A', 'rating': 'N/A', 'title': 'No Data', 'text': 'No pros/cons found'}]\n",
    "\n",
    "# Function for Selenium scraping (dynamic sites)\n",
    "def scrape_with_selenium(url, source, review_locator, parse_func, pages=2):\n",
    "    reviews = []\n",
    "    options = Options()\n",
    "    options.headless = True\n",
    "    driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "    driver.get(url)\n",
    "    time.sleep(5)  # Longer initial load for stability\n",
    "\n",
    "    for page_num in range(pages):\n",
    "        if source == 'Google Maps':\n",
    "            try:\n",
    "                reviews_panel = driver.find_element(By.CSS_SELECTOR, '.m6QErb.DxyBCb.kA9KIf.dS8AEf.XiKgde')\n",
    "                for _ in range(15):  # Further increased scrolls to load more if available\n",
    "                    driver.execute_script(\"arguments[0].scrollBy(0, 2000);\", reviews_panel)\n",
    "                    time.sleep(1.5)\n",
    "            except:\n",
    "                print(\"No reviews panel found on Google Maps\")\n",
    "\n",
    "        try:\n",
    "            review_blocks = driver.find_elements(*review_locator)\n",
    "            if not review_blocks:\n",
    "                print(f\"No reviews found on {source} page {page_num + 1}\")\n",
    "            for block in review_blocks:\n",
    "                try:\n",
    "                    review = parse_func(block)\n",
    "                    review['source'] = source\n",
    "                    reviews.append(review)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error parsing {source} review: {e}\")\n",
    "            # Pagination\n",
    "            if source == 'TripAdvisor':\n",
    "                try:\n",
    "                    next_button = driver.find_element(By.CSS_SELECTOR, '.ui_button.nav.next.primary')\n",
    "                    if next_button and next_button.is_displayed() and next_button.is_enabled():\n",
    "                        next_button.click()\n",
    "                        time.sleep(5)\n",
    "                    else:\n",
    "                        print(f\"No next button or not enabled on TripAdvisor page {page_num + 1}\")\n",
    "                        break\n",
    "                except:\n",
    "                    print(f\"No next button found on TripAdvisor page {page_num + 1}\")\n",
    "                    break\n",
    "            else:\n",
    "                break\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to process {source} page {page_num + 1}: {e}\")\n",
    "            break\n",
    "    driver.quit()\n",
    "    return reviews\n",
    "\n",
    "# Parse function for TripAdvisor (added robustness)\n",
    "def parse_tripadvisor(block):\n",
    "    try:\n",
    "        reviewer_elem = block.find_elements(By.CSS_SELECTOR, '.info_text div:first-child')\n",
    "        reviewer = reviewer_elem[0].text.strip() if reviewer_elem else 'Anonymous'\n",
    "        date_elem = block.find_elements(By.CSS_SELECTOR, '.ratingDate')\n",
    "        date = date_elem[0].get_attribute('title') if date_elem else 'N/A'\n",
    "        rating_elem = block.find_elements(By.CSS_SELECTOR, '.ui_bubble_rating')\n",
    "        rating_class = rating_elem[0].get_attribute('class') if rating_elem else ''\n",
    "        rating = str(int(rating_class.split('_')[-1]) / 10) if rating_class else 'N/A'\n",
    "        title_elem = block.find_elements(By.CSS_SELECTOR, '.noQuotes')\n",
    "        title = title_elem[0].text.strip() if title_elem else 'No Title'\n",
    "        text_elem = block.find_elements(By.CSS_SELECTOR, '.partial_entry')\n",
    "        text = text_elem[0].text.strip() if text_elem else 'No Text'\n",
    "        return {\n",
    "            'reviewer': reviewer,\n",
    "            'date': date,\n",
    "            'rating': rating,\n",
    "            'title': title,\n",
    "            'text': text\n",
    "        }\n",
    "    except:\n",
    "        return {'reviewer': 'N/A', 'date': 'N/A', 'rating': 'N/A', 'title': 'No Data', 'text': 'Parse failed'}\n",
    "\n",
    "# Parse function for Google Maps (unchanged)\n",
    "def parse_google(block):\n",
    "    return {\n",
    "        'reviewer': block.find_element(By.CSS_SELECTOR, '.d4r55').text.strip() if block.find_elements(By.CSS_SELECTOR, '.d4r55') else 'Anonymous',\n",
    "        'date': block.find_element(By.CSS_SELECTOR, '.rsqaWe').text.strip() if block.find_elements(By.CSS_SELECTOR, '.rsqaWe') else 'N/A',\n",
    "        'rating': block.get_attribute('aria-label') if block.find_elements(By.CSS_SELECTOR, '.kvMYJc') else 'N/A',\n",
    "        'title': 'No Title',\n",
    "        'text': block.find_element(By.CSS_SELECTOR, '.wiI7pd').text.strip() if block.find_elements(By.CSS_SELECTOR, '.wiI7pd') else 'No Text'\n",
    "    }\n",
    "\n",
    "# Save to CSV\n",
    "def save_to_csv(reviews, filename='hotel_reviews__1.csv'):\n",
    "    if not reviews:\n",
    "        print(\"No reviews to save.\")\n",
    "        return\n",
    "    keys = reviews[0].keys()\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.DictWriter(f, keys)\n",
    "        writer.writeheader()\n",
    "        writer.writerows(reviews)\n",
    "    print(f\"Saved {len(reviews)} reviews to {filename}\")\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # URLs\n",
    "    booking_url = \"https://www.booking.com/reviews/tn/hotel/business.en-gb.html?label=gen173nr-1FCAEoggI46AdIM1gEaEaIAQGYAQe4AQfIAQzYAQHoAQH4AQKIAgGoAgO4ApvZoMoGwAIB0gIkY2Q5ZGY5ZjMtZWEyNi00NzE5LWI5NjgtMzY4N2E4N2U3M2Q32AIG4AIB&sid=32b2429f18b4624fc09d32f51f4441cc&customer_type=total&hp_nav=0&keep_landing=1&order=featuredreviews&rows=75\"\n",
    "    momondo_url = \"https://www.momondo.com/hotels/tunis-tunis-governorate/Business-Hotel-Tunis.mhd2417013.ksp\"\n",
    "    tripadvisor_url = \"https://www.tripadvisor.com/Hotel_Review-g293758-d8767447-Reviews-Business_Hotel_Tunis-Tunis_Tunis_Governorate.html\"\n",
    "    google_url = \"https://www.google.com/maps/place/Business+H%C3%B4tel/@36.818325,10.1820211,17z/data=!4m8!3m7!1s0x12fd3489b5a4f4e1:0xdda07f445b5b03cd!8m2!3d36.818325!4d10.184596!9m1!1b1!16s%2Fg%2F11bwn563xh?entry=ttu\"\n",
    "    \n",
    "    # Scrape static sites\n",
    "    booking_reviews = scrape_with_bs(booking_url, 'Booking.com', 'div.review_list_new_item_block', parse_booking)\n",
    "    momondo_reviews = scrape_with_bs(momondo_url, 'Momondo', '', parse_momondo)  # Empty selector for soup parsing\n",
    "    \n",
    "    # Scrape dynamic sites\n",
    "    tripadvisor_reviews = scrape_with_selenium(tripadvisor_url, 'TripAdvisor', (By.CLASS_NAME, 'review-container'), parse_tripadvisor)\n",
    "    google_reviews = scrape_with_selenium(google_url, 'Google Maps', (By.CLASS_NAME, 'jftiEf'), parse_google)\n",
    "    \n",
    "    all_reviews = booking_reviews + momondo_reviews + tripadvisor_reviews + google_reviews\n",
    "    save_to_csv(all_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "598037ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Scraping Booking.com ---\n",
      "Fetching Booking.com page 1 from https://www.booking.com/reviews/tn/hotel/business.en-gb.html?label=gen173nr-1FCAEoggI46AdIM1gEaEaIAQGYAQe4AQfIAQzYAQHoAQH4AQKIAgGoAgO4ApvZoMoGwAIB0gIkY2Q5ZGY5ZjMtZWEyNi00NzE5LWI5NjgtMzY4N2E4N2U3M2Q32AIG4AIB&sid=32b2429f18b4624fc09d32f51f4441cc&customer_type=total&hp_nav=0&keep_landing=1&order=featuredreviews&rows=75&page=1\n",
      "Booking.com page 1 fetch status: 200\n",
      "No reviews found on Booking.com first page with selector 'div.c-review-block'.\n",
      "\n",
      "--- Scraping Momondo.com ---\n",
      "Momondo fetch status: 200\n",
      "\n",
      "--- Scraping TripAdvisor.com ---\n",
      "Initial page load for TripAdvisor: https://www.tripadvisor.com/Hotel_Review-g293758-d8767447-Reviews-Business_Hotel_Tunis-Tunis_Tunis_Governorate.html\n",
      "No review blocks found for TripAdvisor with locator ('css selector', 'div[data-reviewid]').\n",
      "Error with TripAdvisor pagination on page 1: Message: no such element: Unable to locate element: {\"method\":\"css selector\",\"selector\":\".ui_button.nav.next.primary\"}\n",
      "  (Session info: chrome=142.0.7444.176); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#nosuchelementexception\n",
      "Stacktrace:\n",
      "Symbols not available. Dumping unresolved backtrace:\n",
      "\t0xd04103\n",
      "\t0xd04144\n",
      "\t0xb0e71d\n",
      "\t0xb5a03d\n",
      "\t0xb5a41b\n",
      "\t0xba17f2\n",
      "\t0xb7c954\n",
      "\t0xb9ee17\n",
      "\t0xb7c706\n",
      "\t0xb4da30\n",
      "\t0xb4ed54\n",
      "\t0xf757b4\n",
      "\t0xf7098a\n",
      "\t0xd2c392\n",
      "\t0xd1c4c8\n",
      "\t0xd2324d\n",
      "\t0xd0c478\n",
      "\t0xd0c63c\n",
      "\t0xcf67ca\n",
      "\t0x756e7ba9\n",
      "\t0x7719c3ab\n",
      "\t0x7719c32f\n",
      "\n",
      "\n",
      "--- Scraping Google Maps ---\n",
      "Initial page load for Google Maps: https://www.google.com/maps/place/Business+H%C3%B4tel/@36.818325,10.1820211,17z/data=!4m8!3m7!1s0x12fd3489b5a4f4e1:0xdda07f445b5b03cd!8m2!3d36.818325!4d10.184596!9m1!1b1!16s%2Fg%2F11bwn563xh?entry=ttu\n",
      "Google Maps reviews panel found.\n",
      "Finished scrolling Google Maps reviews.\n",
      "Saved 209 reviews to hotel_reviews_combined.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time\n",
    "import re # Pour la gestion de l'URL de Booking.com\n",
    "\n",
    "# Enhanced headers to mimic full browser request and avoid 406\n",
    "HEADERS = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',\n",
    "    'Accept-Language': 'en-US,en;q=0.9',\n",
    "    'Accept-Encoding': 'gzip, deflate, br',\n",
    "    'Sec-Fetch-Mode': 'navigate',\n",
    "    'Sec-Fetch-Site': 'none',\n",
    "    'Sec-Fetch-User': '?1',\n",
    "    'Upgrade-Insecure-Requests': '1'\n",
    "}\n",
    "\n",
    "# Function for BeautifulSoup scraping (static sites)\n",
    "def scrape_with_bs(url, source, review_selector, parse_func, pages=2):\n",
    "    reviews = []\n",
    "    \n",
    "    # Momondo is special, it doesn't have review blocks in the same way, and no pagination for \"pros/cons\"\n",
    "    if source == 'Momondo':\n",
    "        response = requests.get(url, headers=HEADERS, allow_redirects=True)\n",
    "        print(f\"{source} fetch status: {response.status_code}\")\n",
    "        if response.status_code not in [200, 301, 302]:\n",
    "            print(f\"Failed to fetch {source} (status: {response.status_code})\")\n",
    "            return []\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        # parse_func for Momondo returns a list of reviews/items\n",
    "        parsed_momondo_reviews = parse_func(soup)\n",
    "        for pr in parsed_momondo_reviews:\n",
    "            pr['source'] = source\n",
    "            reviews.append(pr)\n",
    "        return reviews\n",
    "\n",
    "    # Standard BS scraping for paginated sites like Booking\n",
    "    for page in range(1, pages + 1):\n",
    "        paginated_url = url\n",
    "        if 'booking' in url.lower():\n",
    "            # Robust URL parameter replacement for Booking pagination\n",
    "            if \"page=\" in paginated_url:\n",
    "                paginated_url = re.sub(r\"page=\\d+\", f\"page={page}\", paginated_url)\n",
    "            else:\n",
    "                paginated_url = f\"{paginated_url}&page={page}\"\n",
    "\n",
    "        print(f\"Fetching {source} page {page} from {paginated_url}\")\n",
    "        response = requests.get(paginated_url, headers=HEADERS, allow_redirects=True)\n",
    "        print(f\"{source} page {page} fetch status: {response.status_code}\")\n",
    "        if response.status_code not in [200, 301, 302]:\n",
    "            print(f\"Failed to fetch {source} page {page} (status: {response.status_code})\")\n",
    "            continue\n",
    "        \n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        review_blocks = soup.select(review_selector)\n",
    "\n",
    "        if not review_blocks and page == 1:\n",
    "            print(f\"No reviews found on {source} first page with selector '{review_selector}'.\")\n",
    "            break # No reviews at all\n",
    "        elif not review_blocks:\n",
    "            print(f\"No more reviews on {source} page {page}.\")\n",
    "            break # End of pagination\n",
    "\n",
    "        for block in review_blocks:\n",
    "            try:\n",
    "                review = parse_func(block)\n",
    "                review['source'] = source\n",
    "                reviews.append(review)\n",
    "            except Exception as e:\n",
    "                print(f\"Error parsing {source} review: {e} in block: {block.prettify()[:200]}...\") # Log partial block for debug\n",
    "        \n",
    "        time.sleep(random.uniform(2, 5)) # Be polite\n",
    "\n",
    "    return reviews\n",
    "\n",
    "# Parse function for Booking.com (Potential updated selectors)\n",
    "def parse_booking(block):\n",
    "    # These selectors are examples. YOU MUST VERIFY THEM with browser inspector.\n",
    "    reviewer = block.select_one('.bui-avatar-block__title') # This is for reviewer name\n",
    "    if not reviewer: reviewer = block.select_one('.bui-link--light') # Fallback for reviewer\n",
    "\n",
    "    date = block.select_one('.review_item_date') # Example\n",
    "    rating = block.select_one('.bui-review-score__badge') # Example\n",
    "    title = block.select_one('.c-review-block__title') # Example\n",
    "    \n",
    "    # Booking often has positive and negative parts\n",
    "    positive_text_elem = block.select_one('.c-review__body:nth-of-type(1)') # Adjust if multiple .c-review__body\n",
    "    negative_text_elem = block.select_one('.c-review__body:nth-of-type(2)')\n",
    "    \n",
    "    full_text = []\n",
    "    if positive_text_elem: full_text.append(positive_text_elem.text.strip())\n",
    "    if negative_text_elem: full_text.append(negative_text_elem.text.strip())\n",
    "\n",
    "    return {\n",
    "        'reviewer': reviewer.text.strip() if reviewer else 'Anonymous',\n",
    "        'date': date.text.strip() if date else 'N/A',\n",
    "        'rating': rating.text.strip() if rating else 'N/A',\n",
    "        'title': title.text.strip() if title else 'No Title',\n",
    "        'text': ' '.join(full_text) if full_text else 'No Text'\n",
    "    }\n",
    "\n",
    "# Parse function for Momondo (refined selectors and consistent return)\n",
    "def parse_momondo(soup):\n",
    "    reviews = []\n",
    "    \n",
    "    # Momondo tends to aggregate pros/cons, not individual reviews easily\n",
    "    # It might be better to capture this as one \"aggregated\" review or ignore for individual sentiment analysis.\n",
    "    # For now, we'll try to capture them as separate \"reviews\" in the list.\n",
    "\n",
    "    pros_text = []\n",
    "    pros_h3 = soup.find('h3', string=lambda text: text and 'Pros +' in text.strip())\n",
    "    if pros_h3:\n",
    "        pros_ul = pros_h3.find_next('ul')\n",
    "        if pros_ul:\n",
    "            pros_text = [li.text.strip() for li in pros_ul.find_all('li')]\n",
    "            reviews.append({\n",
    "                'reviewer': 'Momondo Aggregated',\n",
    "                'date': 'N/A',\n",
    "                'rating': 'N/A',\n",
    "                'title': 'Pros of Business Hotel',\n",
    "                'text': '; '.join(pros_text),\n",
    "                'sentiment_label': 'Positive' # Can manually label these\n",
    "            })\n",
    "\n",
    "    cons_text = []\n",
    "    cons_h3 = soup.find('h3', string=lambda text: text and 'Cons -' in text.strip())\n",
    "    if cons_h3:\n",
    "        cons_ul = cons_h3.find_next('ul')\n",
    "        if cons_ul:\n",
    "            cons_text = [li.text.strip() for li in cons_ul.find_all('li')]\n",
    "            reviews.append({\n",
    "                'reviewer': 'Momondo Aggregated',\n",
    "                'date': 'N/A',\n",
    "                'rating': 'N/A',\n",
    "                'title': 'Cons of Business Hotel',\n",
    "                'text': '; '.join(cons_text),\n",
    "                'sentiment_label': 'Negative' # Can manually label these\n",
    "            })\n",
    "    \n",
    "    if not reviews:\n",
    "        reviews.append({'reviewer': 'N/A', 'date': 'N/A', 'rating': 'N/A', 'title': 'No Data', 'text': 'No pros/cons found', 'sentiment_label': 'Neutral'})\n",
    "    \n",
    "    return reviews\n",
    "\n",
    "\n",
    "# Function for Selenium scraping (dynamic sites)\n",
    "def scrape_with_selenium(url, source, review_locator, parse_func, pages=2):\n",
    "    reviews = []\n",
    "    options = Options()\n",
    "    options.headless = True # Run in headless mode\n",
    "    # Fix for newer Chrome versions:\n",
    "    options.add_argument(\"--disable-gpu\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    options.add_argument(f\"user-agent={HEADERS['User-Agent']}\") # Pass User-Agent to Selenium\n",
    "\n",
    "    driver = None\n",
    "    try:\n",
    "        driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "        driver.get(url)\n",
    "        print(f\"Initial page load for {source}: {url}\")\n",
    "        time.sleep(5)  # Longer initial load for stability\n",
    "\n",
    "        # --- Specific logic for Google Maps & TripAdvisor ---\n",
    "        if source == 'Google Maps':\n",
    "            # This is the reviews panel, it's often more reliable to find it by its ARIA role or data-attributes\n",
    "            # Or by its parent container that is scrollable\n",
    "            # This CSS selector (.m6QErb.DxyBCb.kA9KIf.dS8AEf.XiKgde) might be volatile.\n",
    "            # Look for a more stable element, e.g., the div containing all reviews, which has a scrollbar.\n",
    "            try:\n",
    "                # Find the scrollable reviews panel\n",
    "                # This selector is crucial and needs to be verified on Google Maps\n",
    "                reviews_panel = WebDriverWait(driver, 20).until(\n",
    "                    EC.presence_of_element_located((By.CSS_SELECTOR, 'div.m6QErb.DxyBCb.kA9KIf.dS8AEf.XiKgde')) # Verify this\n",
    "                )\n",
    "                print(\"Google Maps reviews panel found.\")\n",
    "                # Scroll multiple times to load all reviews\n",
    "                for _ in range(20): # Increased scrolls for more reviews\n",
    "                    driver.execute_script(\"arguments[0].scrollBy(0, 5000);\", reviews_panel) # Scroll more aggressively\n",
    "                    time.sleep(2) # Increased sleep for more content to load\n",
    "                    # Optional: check if new reviews loaded to stop early\n",
    "                print(\"Finished scrolling Google Maps reviews.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Could not find or scroll Google Maps reviews panel: {e}\")\n",
    "\n",
    "        # TripAdvisor pagination is handled internally by Selenium loop\n",
    "        \n",
    "        # --- Extract reviews after loading/scrolling ---\n",
    "        review_blocks = driver.find_elements(*review_locator)\n",
    "        if not review_blocks:\n",
    "            print(f\"No review blocks found for {source} with locator {review_locator}.\")\n",
    "        \n",
    "        # This loop will now get all currently loaded reviews after scrolling/pagination\n",
    "        for block in review_blocks:\n",
    "            try:\n",
    "                review = parse_func(block)\n",
    "                review['source'] = source\n",
    "                reviews.append(review)\n",
    "            except Exception as e:\n",
    "                print(f\"Error parsing {source} review: {e} in block: {block.text[:100]}...\") # Log partial block\n",
    "\n",
    "        # TripAdvisor specific pagination (if not already handled by the above loop)\n",
    "        if source == 'TripAdvisor':\n",
    "            for page_num in range(pages): # pages here represents \"next\" clicks\n",
    "                try:\n",
    "                    next_button = driver.find_element(By.CSS_SELECTOR, '.ui_button.nav.next.primary') # Verify selector\n",
    "                    if next_button.is_displayed() and next_button.is_enabled():\n",
    "                        next_button.click()\n",
    "                        time.sleep(5) # Wait for new page to load\n",
    "                        \n",
    "                        # Re-find review blocks after navigating to next page\n",
    "                        new_review_blocks = driver.find_elements(*review_locator)\n",
    "                        if not new_review_blocks:\n",
    "                            print(f\"No new reviews found after clicking next on TripAdvisor page {page_num + 1}.\")\n",
    "                            break\n",
    "                        for block in new_review_blocks:\n",
    "                            try:\n",
    "                                review = parse_func(block)\n",
    "                                review['source'] = source\n",
    "                                reviews.append(review)\n",
    "                            except Exception as e:\n",
    "                                print(f\"Error parsing {source} review: {e} in new block: {block.text[:100]}...\")\n",
    "                    else:\n",
    "                        print(f\"No more next button or not enabled on TripAdvisor after {page_num + 1} pages.\")\n",
    "                        break\n",
    "                except Exception as e:\n",
    "                    print(f\"Error with TripAdvisor pagination on page {page_num + 1}: {e}\")\n",
    "                    break\n",
    "        \n",
    "    finally:\n",
    "        if driver:\n",
    "            driver.quit()\n",
    "    return reviews\n",
    "\n",
    "\n",
    "# Parse function for TripAdvisor (using Selenium WebElement methods)\n",
    "def parse_tripadvisor(block):\n",
    "    # These selectors are examples. YOU MUST VERIFY THEM with browser inspector.\n",
    "    try:\n",
    "        reviewer_elem = block.find_elements(By.CSS_SELECTOR, '.info_text div:first-child') # Example\n",
    "        reviewer = reviewer_elem[0].text.strip() if reviewer_elem else 'Anonymous'\n",
    "        \n",
    "        date_elem = block.find_elements(By.CSS_SELECTOR, '.ratingDate') # Example\n",
    "        date = date_elem[0].get_attribute('title') if date_elem else 'N/A' # Date is often in title attribute\n",
    "        \n",
    "        rating_elem = block.find_elements(By.CSS_SELECTOR, '.ui_bubble_rating') # Example\n",
    "        rating = 'N/A'\n",
    "        if rating_elem:\n",
    "            # Rating is often in a class like 'bubble_50' for 5 stars, 'bubble_40' for 4 stars\n",
    "            for cls in rating_elem[0].get_attribute('class').split():\n",
    "                if 'bubble_' in cls:\n",
    "                    rating = str(int(cls.replace('bubble_', '')) / 10.0) # Convert 'bubble_50' to '5.0'\n",
    "                    break\n",
    "\n",
    "        title_elem = block.find_elements(By.CSS_SELECTOR, '.noQuotes') # Example\n",
    "        title = title_elem[0].text.strip() if title_elem else 'No Title'\n",
    "        \n",
    "        text_elem = block.find_elements(By.CSS_SELECTOR, '.partial_entry') # Example, might need to click \"more\"\n",
    "        text = text_elem[0].text.strip() if text_elem else 'No Text'\n",
    "        \n",
    "        return {\n",
    "            'reviewer': reviewer,\n",
    "            'date': date,\n",
    "            'rating': rating,\n",
    "            'title': title,\n",
    "            'text': text\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"TripAdvisor parse error: {e}. Block text: {block.text[:100]}...\")\n",
    "        return {'reviewer': 'N/A', 'date': 'N/A', 'rating': 'N/A', 'title': 'No Data', 'text': 'Parse failed'}\n",
    "\n",
    "# Parse function for Google Maps (using Selenium WebElement methods)\n",
    "def parse_google(block):\n",
    "    # These selectors are examples. YOU MUST VERIFY THEM with browser inspector.\n",
    "    try:\n",
    "        # Reviewer name\n",
    "        reviewer_elem = block.find_elements(By.CSS_SELECTOR, '.d4r55') # Example\n",
    "        reviewer = reviewer_elem[0].text.strip() if reviewer_elem else 'Anonymous'\n",
    "\n",
    "        # Date\n",
    "        date_elem = block.find_elements(By.CSS_SELECTOR, '.rsqaWe') # Example\n",
    "        date = date_elem[0].text.strip() if date_elem else 'N/A'\n",
    "        \n",
    "        # Rating - often an aria-label on a star element or its parent\n",
    "        rating_elem = block.find_elements(By.CSS_SELECTOR, '.kvMYJc') # Example: the div containing stars\n",
    "        rating = 'N/A'\n",
    "        if rating_elem:\n",
    "            rating_label = rating_elem[0].get_attribute('aria-label') # \"Note 5 sur 5\"\n",
    "            if rating_label and 'sur' in rating_label:\n",
    "                rating = rating_label.split(' ')[1] # Extract '5'\n",
    "        \n",
    "        # Full review text\n",
    "        text_elem = block.find_elements(By.CSS_SELECTOR, '.wiI7pd') # Example\n",
    "        text = text_elem[0].text.strip() if text_elem else 'No Text'\n",
    "\n",
    "        return {\n",
    "            'reviewer': reviewer,\n",
    "            'date': date,\n",
    "            'rating': rating,\n",
    "            'title': 'No Title (Google Maps)', # Google Maps reviews typically don't have titles\n",
    "            'text': text\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Google Maps parse error: {e}. Block text: {block.text[:100]}...\")\n",
    "        return {'reviewer': 'N/A', 'date': 'N/A', 'rating': 'N/A', 'title': 'No Data', 'text': 'Parse failed'}\n",
    "\n",
    "\n",
    "# Save to CSV\n",
    "def save_to_csv(reviews, filename='hotel_reviews_combined.csv'): # Changed filename\n",
    "    if not reviews:\n",
    "        print(\"No reviews to save.\")\n",
    "        return\n",
    "    # Ensure all review dictionaries have the same keys for DictWriter\n",
    "    # Create a superset of all keys found across all reviews\n",
    "    all_keys = set()\n",
    "    for review in reviews:\n",
    "        all_keys.update(review.keys())\n",
    "    \n",
    "    # Fill missing keys with None for consistency\n",
    "    reviews_for_csv = []\n",
    "    for review in reviews:\n",
    "        full_review = {key: review.get(key) for key in all_keys}\n",
    "        reviews_for_csv.append(full_review)\n",
    "\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.DictWriter(f, sorted(list(all_keys))) # Sort keys for consistent column order\n",
    "        writer.writeheader()\n",
    "        writer.writerows(reviews_for_csv)\n",
    "    print(f\"Saved {len(reviews)} reviews to {filename}\")\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # URLs\n",
    "    booking_url = \"https://www.booking.com/reviews/tn/hotel/business.en-gb.html?label=gen173nr-1FCAEoggI46AdIM1gEaEaIAQGYAQe4AQfIAQzYAQHoAQH4AQKIAgGoAgO4ApvZoMoGwAIB0gIkY2Q5ZGY5ZjMtZWEyNi00NzE5LWI5NjgtMzY4N2E4N2U3M2Q32AIG4AIB&sid=32b2429f18b4624fc09d32f51f4441cc&customer_type=total&hp_nav=0&keep_landing=1&order=featuredreviews&rows=75\"\n",
    "    momondo_url = \"https://www.momondo.com/hotels/tunis-tunis-governorate/Business-Hotel-Tunis.mhd2417013.ksp\"\n",
    "    tripadvisor_url = \"https://www.tripadvisor.com/Hotel_Review-g293758-d8767447-Reviews-Business_Hotel_Tunis-Tunis_Tunis_Governorate.html\"\n",
    "    google_url = \"https://www.google.com/maps/place/Business+H%C3%B4tel/@36.818325,10.1820211,17z/data=!4m8!3m7!1s0x12fd3489b5a4f4e1:0xdda07f445b5b03cd!8m2!3d36.818325!4d10.184596!9m1!1b1!16s%2Fg%2F11bwn563xh?entry=ttu\"\n",
    "    \n",
    "    all_reviews = []\n",
    "\n",
    "    print(\"\\n--- Scraping Booking.com ---\")\n",
    "    # Increased pages for Booking (75 reviews/page * 20 pages = 1500 potential reviews)\n",
    "    booking_reviews = scrape_with_bs(booking_url, 'Booking.com', 'div.c-review-block', parse_booking, pages=20) \n",
    "    all_reviews.extend(booking_reviews)\n",
    "    \n",
    "    print(\"\\n--- Scraping Momondo.com ---\")\n",
    "    momondo_reviews = scrape_with_bs(momondo_url, 'Momondo', '', parse_momondo, pages=1) # Momondo is one-off\n",
    "    all_reviews.extend(momondo_reviews)\n",
    "    \n",
    "    print(\"\\n--- Scraping TripAdvisor.com ---\")\n",
    "    # TripAdvisor pages count how many \"next\" clicks to simulate\n",
    "    tripadvisor_reviews = scrape_with_selenium(tripadvisor_url, 'TripAdvisor', (By.CSS_SELECTOR, 'div[data-reviewid]'), parse_tripadvisor, pages=10) \n",
    "    all_reviews.extend(tripadvisor_reviews)\n",
    "\n",
    "    print(\"\\n--- Scraping Google Maps ---\")\n",
    "    # Google Maps needs more scrolls than pages, so 'pages' here could mean \"how many times to try and scroll and extract\"\n",
    "    google_reviews = scrape_with_selenium(google_url, 'Google Maps', (By.CSS_SELECTOR, 'div.jftiEf'), parse_google, pages=1) # pages here refers to how many times to execute the scroll loop\n",
    "    all_reviews.extend(google_reviews)\n",
    "    \n",
    "    save_to_csv(all_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c075e45c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Scraping Booking.com ---\n",
      "Fetching Booking.com page 1 from https://www.booking.com/reviews/tn/hotel/business.en-gb.html?label=gen173nr-1FCAEoggI46AdIM1gEaEaIAQGYAQe4AQfIAQzYAQHoAQH4AQKIAgGoAgO4ApvZoMoGwAIB0gIkY2Q5ZGY5ZjMtZWEyNi00NzE5LWI5NjgtMzY4N2E4N2U3M2Q32AIG4AIB&sid=32b2429f18b4624fc09d32f51f4441cc&customer_type=total&hp_nav=0&keep_landing=1&order=featuredreviews&rows=75&page=1\n",
      "Booking.com page 1 fetch status: 200\n",
      "No reviews found on Booking.com first page with selector 'div.review_list_new_item_block'.\n",
      "\n",
      "--- Scraping Momondo.com ---\n",
      "Momondo fetch status: 200\n",
      "\n",
      "--- Scraping TripAdvisor.com ---\n",
      "Initial page load for TripAdvisor: https://www.tripadvisor.com/Hotel_Review-g293758-d8767447-Reviews-Business_Hotel_Tunis-Tunis_Tunis_Governorate.html\n",
      "Error with TripAdvisor pagination on page 1: Message: no such element: Unable to locate element: {\"method\":\"css selector\",\"selector\":\".ui_button.nav.next.primary\"}\n",
      "  (Session info: chrome=142.0.7444.176); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#nosuchelementexception\n",
      "Stacktrace:\n",
      "Symbols not available. Dumping unresolved backtrace:\n",
      "\t0xbb4103\n",
      "\t0xbb4144\n",
      "\t0x9be71d\n",
      "\t0xa0a03d\n",
      "\t0xa0a41b\n",
      "\t0xa517f2\n",
      "\t0xa2c954\n",
      "\t0xa4ee17\n",
      "\t0xa2c706\n",
      "\t0x9fda30\n",
      "\t0x9fed54\n",
      "\t0xe257b4\n",
      "\t0xe2098a\n",
      "\t0xbdc392\n",
      "\t0xbcc4c8\n",
      "\t0xbd324d\n",
      "\t0xbbc478\n",
      "\t0xbbc63c\n",
      "\t0xba67ca\n",
      "\t0x756e7ba9\n",
      "\t0x7719c3ab\n",
      "\t0x7719c32f\n",
      "\n",
      "No review blocks found for TripAdvisor with locator ('class name', 'review-container').\n",
      "\n",
      "--- Scraping Google Maps ---\n",
      "Initial page load for Google Maps: https://www.google.com/maps/place/Business+H%C3%B4tel/@36.818325,10.1820211,17z/data=!4m8!3m7!1s0x12fd3489b5a4f4e1:0xdda07f445b5b03cd!8m2!3d36.818325!4d10.184596!9m1!1b1!16s%2Fg%2F11bwn563xh?entry=ttu\n",
      "Google Maps reviews panel found.\n",
      "Finished scrolling Google Maps reviews.\n",
      "Saved 209 reviews to hotel_reviews_combined11.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time\n",
    "import re  # Pour la gestion de l'URL de Booking.com\n",
    "import random  # For polite delays\n",
    "\n",
    "# Enhanced headers to mimic full browser request and avoid 406\n",
    "HEADERS = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',\n",
    "    'Accept-Language': 'en-US,en;q=0.9',\n",
    "    'Accept-Encoding': 'gzip, deflate, br',\n",
    "    'Sec-Fetch-Mode': 'navigate',\n",
    "    'Sec-Fetch-Site': 'none',\n",
    "    'Sec-Fetch-User': '?1',\n",
    "    'Upgrade-Insecure-Requests': '1'\n",
    "}\n",
    "\n",
    "# Function for BeautifulSoup scraping (static sites)\n",
    "def scrape_with_bs(url, source, review_selector, parse_func, pages=2):\n",
    "    reviews = []\n",
    "    \n",
    "    # Momondo is special, it doesn't have review blocks in the same way, and no pagination for \"pros/cons\"\n",
    "    if source == 'Momondo':\n",
    "        response = requests.get(url, headers=HEADERS, allow_redirects=True)\n",
    "        print(f\"{source} fetch status: {response.status_code}\")\n",
    "        if response.status_code not in [200, 301, 302]:\n",
    "            print(f\"Failed to fetch {source} (status: {response.status_code})\")\n",
    "            return []\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        # parse_func for Momondo returns a list of reviews/items\n",
    "        parsed_momondo_reviews = parse_func(soup)\n",
    "        for pr in parsed_momondo_reviews:\n",
    "            pr['source'] = source\n",
    "            reviews.append(pr)\n",
    "        return reviews\n",
    "    \n",
    "    # Standard BS scraping for paginated sites like Booking\n",
    "    for page in range(1, pages + 1):\n",
    "        paginated_url = url\n",
    "        if 'booking' in url.lower():\n",
    "            # Robust URL parameter replacement for Booking pagination\n",
    "            if \"page=\" in paginated_url:\n",
    "                paginated_url = re.sub(r\"page=\\d+\", f\"page={page}\", paginated_url)\n",
    "            else:\n",
    "                paginated_url = f\"{paginated_url}&page={page}\"\n",
    "        print(f\"Fetching {source} page {page} from {paginated_url}\")\n",
    "        response = requests.get(paginated_url, headers=HEADERS, allow_redirects=True)\n",
    "        print(f\"{source} page {page} fetch status: {response.status_code}\")\n",
    "        if response.status_code not in [200, 301, 302]:\n",
    "            print(f\"Failed to fetch {source} page {page} (status: {response.status_code})\")\n",
    "            continue\n",
    "        \n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        review_blocks = soup.select(review_selector)\n",
    "        if not review_blocks and page == 1:\n",
    "            print(f\"No reviews found on {source} first page with selector '{review_selector}'.\")\n",
    "            break  # No reviews at all\n",
    "        elif not review_blocks:\n",
    "            print(f\"No more reviews on {source} page {page}.\")\n",
    "            break  # End of pagination\n",
    "        for block in review_blocks:\n",
    "            try:\n",
    "                review = parse_func(block)\n",
    "                review['source'] = source\n",
    "                reviews.append(review)\n",
    "            except Exception as e:\n",
    "                print(f\"Error parsing {source} review: {e} in block: {block.prettify()[:200]}...\")  # Log partial block for debug\n",
    "        \n",
    "        time.sleep(random.uniform(2, 5))  # Be polite\n",
    "    return reviews\n",
    "\n",
    "# Parse function for Booking.com (Updated selectors based on site inspection)\n",
    "def parse_booking(block):\n",
    "    # Updated selectors from site structure\n",
    "    reviewer = block.select_one('span.bui-avatar-block__title')  # Reviewer name\n",
    "    date = block.select_one('span.c-review-block__date')  # Date\n",
    "    rating = block.select_one('div.bui-review-score__badge')  # Rating\n",
    "    title = block.select_one('h3.c-review-block__title')  # Title\n",
    "    \n",
    "    # Booking often has positive and negative parts\n",
    "    positive_text_elem = block.select('span.c-review__body--original')\n",
    "    negative_text_elem = block.select('span.c-review__body')\n",
    "    \n",
    "    full_text = []\n",
    "    if positive_text_elem:\n",
    "        full_text.extend([elem.text.strip() for elem in positive_text_elem])\n",
    "    if negative_text_elem:\n",
    "        full_text.extend([elem.text.strip() for elem in negative_text_elem if elem not in positive_text_elem])  # Avoid duplicates\n",
    "    return {\n",
    "        'reviewer': reviewer.text.strip() if reviewer else 'Anonymous',\n",
    "        'date': date.text.strip() if date else 'N/A',\n",
    "        'rating': rating.text.strip() if rating else 'N/A',\n",
    "        'title': title.text.strip() if title else 'No Title',\n",
    "        'text': ' '.join(full_text) if full_text else 'No Text'\n",
    "    }\n",
    "\n",
    "# Parse function for Momondo (refined to handle h3 or h6 headings)\n",
    "def parse_momondo(soup):\n",
    "    reviews = []\n",
    "    \n",
    "    # Flexible heading search for Pros +\n",
    "    pros_heading = soup.find(lambda tag: tag.name in ['h3', 'h6'] and 'Pros +' in tag.text.strip())\n",
    "    if pros_heading:\n",
    "        pros_ul = pros_heading.find_next('ul')\n",
    "        if pros_ul:\n",
    "            items = [li.text.strip() for li in pros_ul.find_all('li')]\n",
    "            reviews.append({\n",
    "                'reviewer': 'Momondo Aggregated',\n",
    "                'date': 'N/A',\n",
    "                'rating': 'N/A',\n",
    "                'title': 'Pros of Business Hotel',\n",
    "                'text': '; '.join(items),\n",
    "                'sentiment_label': 'Positive'  # Can manually label these\n",
    "            })\n",
    "    \n",
    "    # Flexible heading search for Cons -\n",
    "    cons_heading = soup.find(lambda tag: tag.name in ['h3', 'h6'] and 'Cons -' in tag.text.strip())\n",
    "    if cons_heading:\n",
    "        cons_ul = cons_heading.find_next('ul')\n",
    "        if cons_ul:\n",
    "            items = [li.text.strip() for li in cons_ul.find_all('li')]\n",
    "            reviews.append({\n",
    "                'reviewer': 'Momondo Aggregated',\n",
    "                'date': 'N/A',\n",
    "                'rating': 'N/A',\n",
    "                'title': 'Cons of Business Hotel',\n",
    "                'text': '; '.join(items),\n",
    "                'sentiment_label': 'Negative'  # Can manually label these\n",
    "            })\n",
    "    \n",
    "    if not reviews:\n",
    "        reviews.append({'reviewer': 'N/A', 'date': 'N/A', 'rating': 'N/A', 'title': 'No Data', 'text': 'No pros/cons found', 'sentiment_label': 'Neutral'})\n",
    "    \n",
    "    return reviews\n",
    "\n",
    "# Function for Selenium scraping (dynamic sites)\n",
    "def scrape_with_selenium(url, source, review_locator, parse_func, pages=2):\n",
    "    reviews = []\n",
    "    options = Options()\n",
    "    options.headless = True  # Run in headless mode\n",
    "    # Fix for newer Chrome versions:\n",
    "    options.add_argument(\"--disable-gpu\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    options.add_argument(f\"user-agent={HEADERS['User-Agent']}\")  # Pass User-Agent to Selenium\n",
    "    driver = None\n",
    "    try:\n",
    "        driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "        driver.get(url)\n",
    "        print(f\"Initial page load for {source}: {url}\")\n",
    "        time.sleep(5)  # Longer initial load for stability\n",
    "        # --- Specific logic for Google Maps & TripAdvisor ---\n",
    "        if source == 'Google Maps':\n",
    "            try:\n",
    "                # Find the scrollable reviews panel\n",
    "                reviews_panel = WebDriverWait(driver, 20).until(\n",
    "                    EC.presence_of_element_located((By.CSS_SELECTOR, 'div.m6QErb.DxyBCb.kA9KIf.dS8AEf.XiKgde'))  # Verify this\n",
    "                )\n",
    "                print(\"Google Maps reviews panel found.\")\n",
    "                # Scroll multiple times to load all reviews\n",
    "                for _ in range(20):  # Increased scrolls for more reviews\n",
    "                    driver.execute_script(\"arguments[0].scrollBy(0, 5000);\", reviews_panel)  # Scroll more aggressively\n",
    "                    time.sleep(2)  # Increased sleep for more content to load\n",
    "                    # Optional: check if new reviews loaded to stop early\n",
    "                print(\"Finished scrolling Google Maps reviews.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Could not find or scroll Google Maps reviews panel: {e}\")\n",
    "        # TripAdvisor specific pagination (if not already handled by the above loop)\n",
    "        if source == 'TripAdvisor':\n",
    "            for page_num in range(pages):  # pages here represents \"next\" clicks\n",
    "                try:\n",
    "                    next_button = driver.find_element(By.CSS_SELECTOR, '.ui_button.nav.next.primary')  # Verify selector\n",
    "                    if next_button.is_displayed() and next_button.is_enabled():\n",
    "                        next_button.click()\n",
    "                        time.sleep(5)  # Wait for new page to load\n",
    "                        \n",
    "                        # Re-find review blocks after navigating to next page\n",
    "                        new_review_blocks = driver.find_elements(*review_locator)\n",
    "                        if not new_review_blocks:\n",
    "                            print(f\"No new reviews found after clicking next on TripAdvisor page {page_num + 1}.\")\n",
    "                            break\n",
    "                        for block in new_review_blocks:\n",
    "                            try:\n",
    "                                review = parse_func(block)\n",
    "                                review['source'] = source\n",
    "                                reviews.append(review)\n",
    "                            except Exception as e:\n",
    "                                print(f\"Error parsing {source} review: {e} in new block: {block.text[:100]}...\")\n",
    "                    else:\n",
    "                        print(f\"No more next button or not enabled on TripAdvisor after {page_num + 1} pages.\")\n",
    "                        break\n",
    "                except Exception as e:\n",
    "                    print(f\"Error with TripAdvisor pagination on page {page_num + 1}: {e}\")\n",
    "                    break\n",
    "        \n",
    "        # --- Extract reviews after loading/scrolling ---\n",
    "        review_blocks = driver.find_elements(*review_locator)\n",
    "        if not review_blocks:\n",
    "            print(f\"No review blocks found for {source} with locator {review_locator}.\")\n",
    "        \n",
    "        # This loop will now get all currently loaded reviews after scrolling/pagination\n",
    "        for block in review_blocks:\n",
    "            try:\n",
    "                review = parse_func(block)\n",
    "                review['source'] = source\n",
    "                reviews.append(review)\n",
    "            except Exception as e:\n",
    "                print(f\"Error parsing {source} review: {e} in block: {block.text[:100]}...\")  # Log partial block\n",
    "        \n",
    "    finally:\n",
    "        if driver:\n",
    "            driver.quit()\n",
    "    return reviews\n",
    "\n",
    "# Parse function for TripAdvisor (using Selenium WebElement methods)\n",
    "def parse_tripadvisor(block):\n",
    "    try:\n",
    "        reviewer_elem = block.find_elements(By.CSS_SELECTOR, '.info_text div:first-child')  # Example\n",
    "        reviewer = reviewer_elem[0].text.strip() if reviewer_elem else 'Anonymous'\n",
    "        \n",
    "        date_elem = block.find_elements(By.CSS_SELECTOR, '.ratingDate')  # Example\n",
    "        date = date_elem[0].get_attribute('title') if date_elem else 'N/A'  # Date is often in title attribute\n",
    "        \n",
    "        rating_elem = block.find_elements(By.CSS_SELECTOR, '.ui_bubble_rating')  # Example\n",
    "        rating = 'N/A'\n",
    "        if rating_elem:\n",
    "            # Rating is often in a class like 'bubble_50' for 5 stars, 'bubble_40' for 4 stars\n",
    "            for cls in rating_elem[0].get_attribute('class').split():\n",
    "                if 'bubble_' in cls:\n",
    "                    rating = str(int(cls.replace('bubble_', '')) / 10.0)  # Convert 'bubble_50' to '5.0'\n",
    "                    break\n",
    "        title_elem = block.find_elements(By.CSS_SELECTOR, '.noQuotes')  # Example\n",
    "        title = title_elem[0].text.strip() if title_elem else 'No Title'\n",
    "        \n",
    "        text_elem = block.find_elements(By.CSS_SELECTOR, '.partial_entry')  # Example, might need to click \"more\"\n",
    "        text = text_elem[0].text.strip() if text_elem else 'No Text'\n",
    "        \n",
    "        return {\n",
    "            'reviewer': reviewer,\n",
    "            'date': date,\n",
    "            'rating': rating,\n",
    "            'title': title,\n",
    "            'text': text\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"TripAdvisor parse error: {e}. Block text: {block.text[:100]}...\")\n",
    "        return {'reviewer': 'N/A', 'date': 'N/A', 'rating': 'N/A', 'title': 'No Data', 'text': 'Parse failed'}\n",
    "\n",
    "# Parse function for Google Maps (using Selenium WebElement methods)\n",
    "def parse_google(block):\n",
    "    try:\n",
    "        # Reviewer name\n",
    "        reviewer_elem = block.find_elements(By.CSS_SELECTOR, '.d4r55')  # Example\n",
    "        reviewer = reviewer_elem[0].text.strip() if reviewer_elem else 'Anonymous'\n",
    "        # Date\n",
    "        date_elem = block.find_elements(By.CSS_SELECTOR, '.rsqaWe')  # Example\n",
    "        date = date_elem[0].text.strip() if date_elem else 'N/A'\n",
    "        \n",
    "        # Rating - often an aria-label on a star element or its parent\n",
    "        rating_elem = block.find_elements(By.CSS_SELECTOR, '.kvMYJc')  # Example: the div containing stars\n",
    "        rating = 'N/A'\n",
    "        if rating_elem:\n",
    "            rating_label = rating_elem[0].get_attribute('aria-label')  # \"Note 5 sur 5\"\n",
    "            if rating_label and 'sur' in rating_label:\n",
    "                rating = rating_label.split(' ')[1]  # Extract '5'\n",
    "        \n",
    "        # Full review text\n",
    "        text_elem = block.find_elements(By.CSS_SELECTOR, '.wiI7pd')  # Example\n",
    "        text = text_elem[0].text.strip() if text_elem else 'No Text'\n",
    "        return {\n",
    "            'reviewer': reviewer,\n",
    "            'date': date,\n",
    "            'rating': rating,\n",
    "            'title': 'No Title (Google Maps)',  # Google Maps reviews typically don't have titles\n",
    "            'text': text\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Google Maps parse error: {e}. Block text: {block.text[:100]}...\")\n",
    "        return {'reviewer': 'N/A', 'date': 'N/A', 'rating': 'N/A', 'title': 'No Data', 'text': 'Parse failed'}\n",
    "\n",
    "# Save to CSV\n",
    "def save_to_csv(reviews, filename='hotel_reviews_combined11.csv'):  # Changed filename\n",
    "    if not reviews:\n",
    "        print(\"No reviews to save.\")\n",
    "        return\n",
    "    # Ensure all review dictionaries have the same keys for DictWriter\n",
    "    # Create a superset of all keys found across all reviews\n",
    "    all_keys = set()\n",
    "    for review in reviews:\n",
    "        all_keys.update(review.keys())\n",
    "    \n",
    "    # Fill missing keys with None for consistency\n",
    "    reviews_for_csv = []\n",
    "    for review in reviews:\n",
    "        full_review = {key: review.get(key) for key in all_keys}\n",
    "        reviews_for_csv.append(full_review)\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.DictWriter(f, sorted(list(all_keys)))  # Sort keys for consistent column order\n",
    "        writer.writeheader()\n",
    "        writer.writerows(reviews_for_csv)\n",
    "    print(f\"Saved {len(reviews)} reviews to {filename}\")\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # URLs\n",
    "    booking_url = \"https://www.booking.com/reviews/tn/hotel/business.en-gb.html?label=gen173nr-1FCAEoggI46AdIM1gEaEaIAQGYAQe4AQfIAQzYAQHoAQH4AQKIAgGoAgO4ApvZoMoGwAIB0gIkY2Q5ZGY5ZjMtZWEyNi00NzE5LWI5NjgtMzY4N2E4N2U3M2Q32AIG4AIB&sid=32b2429f18b4624fc09d32f51f4441cc&customer_type=total&hp_nav=0&keep_landing=1&order=featuredreviews&rows=75\"\n",
    "    momondo_url = \"https://www.momondo.com/hotels/tunis-tunis-governorate/Business-Hotel-Tunis.mhd2417013.ksp\"\n",
    "    tripadvisor_url = \"https://www.tripadvisor.com/Hotel_Review-g293758-d8767447-Reviews-Business_Hotel_Tunis-Tunis_Tunis_Governorate.html\"\n",
    "    google_url = \"https://www.google.com/maps/place/Business+H%C3%B4tel/@36.818325,10.1820211,17z/data=!4m8!3m7!1s0x12fd3489b5a4f4e1:0xdda07f445b5b03cd!8m2!3d36.818325!4d10.184596!9m1!1b1!16s%2Fg%2F11bwn563xh?entry=ttu\"\n",
    "    \n",
    "    all_reviews = []\n",
    "    print(\"\\n--- Scraping Booking.com ---\")\n",
    "    # Increased pages for Booking (75 reviews/page * 20 pages = 1500 potential reviews)\n",
    "    booking_reviews = scrape_with_bs(booking_url, 'Booking.com', 'div.review_list_new_item_block', parse_booking, pages=20)\n",
    "    all_reviews.extend(booking_reviews)\n",
    "    \n",
    "    print(\"\\n--- Scraping Momondo.com ---\")\n",
    "    momondo_reviews = scrape_with_bs(momondo_url, 'Momondo', '', parse_momondo, pages=1)  # Momondo is one-off\n",
    "    all_reviews.extend(momondo_reviews)\n",
    "    \n",
    "    print(\"\\n--- Scraping TripAdvisor.com ---\")\n",
    "    # TripAdvisor pages count how many \"next\" clicks to simulate\n",
    "    tripadvisor_reviews = scrape_with_selenium(tripadvisor_url, 'TripAdvisor', (By.CLASS_NAME, 'review-container'), parse_tripadvisor, pages=10)\n",
    "    all_reviews.extend(tripadvisor_reviews)\n",
    "    \n",
    "    print(\"\\n--- Scraping Google Maps ---\")\n",
    "    # Google Maps needs more scrolls than pages, so 'pages' here could mean \"how many times to try and scroll and extract\"\n",
    "    google_reviews = scrape_with_selenium(google_url, 'Google Maps', (By.CLASS_NAME, 'jftiEf'), parse_google, pages=1)  # pages here refers to how many times to execute the scroll loop\n",
    "    all_reviews.extend(google_reviews)\n",
    "    \n",
    "    save_to_csv(all_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10dd087",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_Cours",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
