{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3c948a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting requests\n",
      "  Downloading requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
      "Collecting beautifulsoup4\n",
      "  Downloading beautifulsoup4-4.14.2-py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: pandas in c:\\users\\user\\miniconda3\\envs\\ml_cours\\lib\\site-packages (2.3.3)\n",
      "Collecting charset_normalizer<4,>=2 (from requests)\n",
      "  Downloading charset_normalizer-3.4.4-cp313-cp313-win_amd64.whl.metadata (38 kB)\n",
      "Collecting idna<4,>=2.5 (from requests)\n",
      "  Downloading idna-3.11-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests)\n",
      "  Using cached urllib3-2.5.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests)\n",
      "  Downloading certifi-2025.11.12-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4)\n",
      "  Downloading soupsieve-2.8-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\user\\miniconda3\\envs\\ml_cours\\lib\\site-packages (from beautifulsoup4) (4.15.0)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\user\\miniconda3\\envs\\ml_cours\\lib\\site-packages (from pandas) (2.3.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\user\\miniconda3\\envs\\ml_cours\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\user\\miniconda3\\envs\\ml_cours\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\user\\miniconda3\\envs\\ml_cours\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\miniconda3\\envs\\ml_cours\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Downloading requests-2.32.5-py3-none-any.whl (64 kB)\n",
      "Downloading charset_normalizer-3.4.4-cp313-cp313-win_amd64.whl (107 kB)\n",
      "Downloading idna-3.11-py3-none-any.whl (71 kB)\n",
      "Using cached urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
      "Downloading beautifulsoup4-4.14.2-py3-none-any.whl (106 kB)\n",
      "Downloading certifi-2025.11.12-py3-none-any.whl (159 kB)\n",
      "Downloading soupsieve-2.8-py3-none-any.whl (36 kB)\n",
      "Installing collected packages: urllib3, soupsieve, idna, charset_normalizer, certifi, requests, beautifulsoup4\n",
      "\n",
      "   ---------------------------------------- 0/7 [urllib3]\n",
      "   ----------- ---------------------------- 2/7 [idna]\n",
      "   ----------------- ---------------------- 3/7 [charset_normalizer]\n",
      "   ---------------------------- ----------- 5/7 [requests]\n",
      "   ---------------------------------------- 7/7 [beautifulsoup4]\n",
      "\n",
      "Successfully installed beautifulsoup4-4.14.2 certifi-2025.11.12 charset_normalizer-3.4.4 idna-3.11 requests-2.32.5 soupsieve-2.8 urllib3-2.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip install requests beautifulsoup4 pandas\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48809cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60ac36f",
   "metadata": {},
   "source": [
    "# Fct  de scrapping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71eea01f",
   "metadata": {},
   "source": [
    "# =>\n",
    "Le problème rencontré avec Beautiful Soup est typique car de nombreux sites, y compris ceux comme Orange et Ooredoo en Tunisie, masquent les avis clients via des techniques dynamiques (JavaScript, chargement asynchrone, données JSON cachées) qui ne sont pas accessibles par un simple parsing HTML."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a118b613",
   "metadata": {},
   "source": [
    "Solutions pour contourner ce problème\n",
    "Extraction via données JSON cachées : Certains sites stockent les avis dans des structures JSON invisibles directement dans le code source, qui peuvent être extraites après avoir identifié les bons endpoints ou structures. Par exemple, des sites comme Tripadvisor utilisent cette méthode pour leurs avis clients.\n",
    "\n",
    "Scraping dynamique avec Selenium ou Puppeteer : Ces outils simulent un navigateur complet qui exécute le JavaScript afin de charger entièrement la page et ses avis avant extraction.\n",
    "\n",
    "Utilisation d'APIs ou sources alternatives : Parfois, les plateformes ont des APIs publiques ou semi-publiques pour accéder aux avis clients, ou bien on peut extraire les avis depuis des sites tiers comme Trustpilot qui référencent les avis des clients pour ces opérateurs.\n",
    "\n",
    "Automatisation via plateformes comme Make.com + Apify : Certaines plateformes proposent des modules déjà prêts pour collecter des avis de sites comme Trustpilot, avec des options d'analyse, ce qui pourrait faciliter la collecte.\n",
    "\n",
    "Exemples spécifiques aux opérateurs en Tunisie\n",
    "Avis pour Orange Tunisie peuvent être trouvés sur Tripadvisor et Trustpilot mais pas facilement scrapés via BeautifulSoup à cause du contenu dynamique.​\n",
    "\n",
    "Avis pour Ooredoo Tunisie existent aussi sur Trustpilot et Indeed mais de même, nécessitent des méthodes avancées de scraping ou API.​\n",
    "\n",
    "Tutoriels sur le scraping d'avis clients montrent que juste BeautifulSoup est souvent insuffisant pour scraper tous les avis car il faut gérer la partie JavaScript ou JSON cachée.​\n",
    "\n",
    "Pour notre mini-projet on prefere opter pour  : notamment Selenium pour le scraping dynamique, ou bien l'extraction directe de JSON cachés, ou encore d'utiliser des sources comme Trustpilot pour l'analyse des avis clients."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b659bf33",
   "metadata": {},
   "source": [
    "option 1:Selenium\n",
    "\n",
    "Simule un vrai navigateur\n",
    "\n",
    "Charge le JS → les avis deviennent visibles\n",
    "\n",
    "Compatible avec Tunisianet / Orange / Ooredoo\n",
    "\n",
    "Cette méthode est la plus sûre pour scraper des avis en Tunisie aujourd’hui\n",
    "\n",
    "\n",
    "\n",
    "Option 2 : Extraire des avis depuis les réseaux sociaux ou forums\n",
    "\n",
    "Facebook, Twitter, forums de tech tunisiens\n",
    "\n",
    "Souvent visibles sans JS compliqué\n",
    "\n",
    "Mais nécessite un peu de nettoyage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059e54ba",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1fde2562",
   "metadata": {},
   "source": [
    "I WILL WORK ON BUSINESS HOTEL\n",
    "\n",
    "MAPS\n",
    "\n",
    "avis dans site de booking\n",
    "\n",
    "avis de site de rating momondo \n",
    "\n",
    "avid de site de travipadvisdor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "598037ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Scraping Booking.com ---\n",
      "Fetching Booking.com page 1 from https://www.booking.com/reviews/tn/hotel/business.en-gb.html?label=gen173nr-1FCAEoggI46AdIM1gEaEaIAQGYAQe4AQfIAQzYAQHoAQH4AQKIAgGoAgO4ApvZoMoGwAIB0gIkY2Q5ZGY5ZjMtZWEyNi00NzE5LWI5NjgtMzY4N2E4N2U3M2Q32AIG4AIB&sid=32b2429f18b4624fc09d32f51f4441cc&customer_type=total&hp_nav=0&keep_landing=1&order=featuredreviews&rows=75&page=1\n",
      "Booking.com page 1 fetch status: 200\n",
      "No reviews found on Booking.com first page with selector 'div.c-review-block'.\n",
      "\n",
      "--- Scraping Momondo.com ---\n",
      "Momondo fetch status: 200\n",
      "\n",
      "--- Scraping TripAdvisor.com ---\n",
      "Initial page load for TripAdvisor: https://www.tripadvisor.com/Hotel_Review-g293758-d8767447-Reviews-Business_Hotel_Tunis-Tunis_Tunis_Governorate.html\n",
      "No review blocks found for TripAdvisor with locator ('css selector', 'div[data-reviewid]').\n",
      "Error with TripAdvisor pagination on page 1: Message: no such element: Unable to locate element: {\"method\":\"css selector\",\"selector\":\".ui_button.nav.next.primary\"}\n",
      "  (Session info: chrome=142.0.7444.176); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#nosuchelementexception\n",
      "Stacktrace:\n",
      "Symbols not available. Dumping unresolved backtrace:\n",
      "\t0xeb4103\n",
      "\t0xeb4144\n",
      "\t0xcbe71d\n",
      "\t0xd0a03d\n",
      "\t0xd0a41b\n",
      "\t0xd517f2\n",
      "\t0xd2c954\n",
      "\t0xd4ee17\n",
      "\t0xd2c706\n",
      "\t0xcfda30\n",
      "\t0xcfed54\n",
      "\t0x11257b4\n",
      "\t0x112098a\n",
      "\t0xedc392\n",
      "\t0xecc4c8\n",
      "\t0xed324d\n",
      "\t0xebc478\n",
      "\t0xebc63c\n",
      "\t0xea67ca\n",
      "\t0x756e7ba9\n",
      "\t0x7719c3ab\n",
      "\t0x7719c32f\n",
      "\n",
      "\n",
      "--- Scraping Google Maps ---\n",
      "Initial page load for Google Maps: https://www.google.com/maps/place/Business+H%C3%B4tel/@36.818325,10.1820211,17z/data=!4m8!3m7!1s0x12fd3489b5a4f4e1:0xdda07f445b5b03cd!8m2!3d36.818325!4d10.184596!9m1!1b1!16s%2Fg%2F11bwn563xh?entry=ttu\n",
      "Google Maps reviews panel found.\n",
      "Finished scrolling Google Maps reviews.\n",
      "Saved 209 reviews to hotel_reviews_combined__final.csv\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import time\n",
    "import re # Pour la gestion de l'URL de Booking.com\n",
    "\n",
    "# Enhanced headers to mimic full browser request and avoid 406\n",
    "HEADERS = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',\n",
    "    'Accept-Language': 'en-US,en;q=0.9',\n",
    "    'Accept-Encoding': 'gzip, deflate, br',\n",
    "    'Sec-Fetch-Mode': 'navigate',\n",
    "    'Sec-Fetch-Site': 'none',\n",
    "    'Sec-Fetch-User': '?1',\n",
    "    'Upgrade-Insecure-Requests': '1'\n",
    "}\n",
    "\n",
    "# Function for BeautifulSoup scraping (static sites)\n",
    "def scrape_with_bs(url, source, review_selector, parse_func, pages=2):\n",
    "    reviews = []\n",
    "    \n",
    "    # Momondo is special, it doesn't have review blocks in the same way, and no pagination for \"pros/cons\"\n",
    "    if source == 'Momondo':\n",
    "        response = requests.get(url, headers=HEADERS, allow_redirects=True)\n",
    "        print(f\"{source} fetch status: {response.status_code}\")\n",
    "        if response.status_code not in [200, 301, 302]:\n",
    "            print(f\"Failed to fetch {source} (status: {response.status_code})\")\n",
    "            return []\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        # parse_func for Momondo returns a list of reviews/items\n",
    "        parsed_momondo_reviews = parse_func(soup)\n",
    "        for pr in parsed_momondo_reviews:\n",
    "            pr['source'] = source\n",
    "            reviews.append(pr)\n",
    "        return reviews\n",
    "\n",
    "    # Standard BS scraping for paginated sites like Booking\n",
    "    for page in range(1, pages + 1):\n",
    "        paginated_url = url\n",
    "        if 'booking' in url.lower():\n",
    "            # Robust URL parameter replacement for Booking pagination\n",
    "            if \"page=\" in paginated_url:\n",
    "                paginated_url = re.sub(r\"page=\\d+\", f\"page={page}\", paginated_url)\n",
    "            else:\n",
    "                paginated_url = f\"{paginated_url}&page={page}\"\n",
    "\n",
    "        print(f\"Fetching {source} page {page} from {paginated_url}\")\n",
    "        response = requests.get(paginated_url, headers=HEADERS, allow_redirects=True)\n",
    "        print(f\"{source} page {page} fetch status: {response.status_code}\")\n",
    "        if response.status_code not in [200, 301, 302]:\n",
    "            print(f\"Failed to fetch {source} page {page} (status: {response.status_code})\")\n",
    "            continue\n",
    "        \n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        review_blocks = soup.select(review_selector)\n",
    "\n",
    "        if not review_blocks and page == 1:\n",
    "            print(f\"No reviews found on {source} first page with selector '{review_selector}'.\")\n",
    "            break # No reviews at all\n",
    "        elif not review_blocks:\n",
    "            print(f\"No more reviews on {source} page {page}.\")\n",
    "            break # End of pagination\n",
    "\n",
    "        for block in review_blocks:\n",
    "            try:\n",
    "                review = parse_func(block)\n",
    "                review['source'] = source\n",
    "                reviews.append(review)\n",
    "            except Exception as e:\n",
    "                print(f\"Error parsing {source} review: {e} in block: {block.prettify()[:200]}...\") # Log partial block for debug\n",
    "        \n",
    "        time.sleep(random.uniform(2, 5)) # Be polite\n",
    "\n",
    "    return reviews\n",
    "\n",
    "# Parse function for Booking.com (Potential updated selectors)\n",
    "def parse_booking(block):\n",
    "    # These selectors are examples. YOU MUST VERIFY THEM with browser inspector.\n",
    "    reviewer = block.select_one('.bui-avatar-block__title') # This is for reviewer name\n",
    "    if not reviewer: reviewer = block.select_one('.bui-link--light') # Fallback for reviewer\n",
    "\n",
    "    date = block.select_one('.review_item_date') # Example\n",
    "    rating = block.select_one('.bui-review-score__badge') # Example\n",
    "    title = block.select_one('.c-review-block__title') # Example\n",
    "    \n",
    "    # Booking often has positive and negative parts\n",
    "    positive_text_elem = block.select_one('.c-review__body:nth-of-type(1)') # Adjust if multiple .c-review__body\n",
    "    negative_text_elem = block.select_one('.c-review__body:nth-of-type(2)')\n",
    "    \n",
    "    full_text = []\n",
    "    if positive_text_elem: full_text.append(positive_text_elem.text.strip())\n",
    "    if negative_text_elem: full_text.append(negative_text_elem.text.strip())\n",
    "\n",
    "    return {\n",
    "        'reviewer': reviewer.text.strip() if reviewer else 'Anonymous',\n",
    "        'date': date.text.strip() if date else 'N/A',\n",
    "        'rating': rating.text.strip() if rating else 'N/A',\n",
    "        'title': title.text.strip() if title else 'No Title',\n",
    "        'text': ' '.join(full_text) if full_text else 'No Text'\n",
    "    }\n",
    "\n",
    "# Parse function for Momondo (refined selectors and consistent return)\n",
    "def parse_momondo(soup):\n",
    "    reviews = []\n",
    "    \n",
    "    # Momondo tends to aggregate pros/cons, not individual reviews easily\n",
    "    # It might be better to capture this as one \"aggregated\" review or ignore for individual sentiment analysis.\n",
    "    # For now, we'll try to capture them as separate \"reviews\" in the list.\n",
    "\n",
    "    pros_text = []\n",
    "    pros_h3 = soup.find('h3', string=lambda text: text and 'Pros +' in text.strip())\n",
    "    if pros_h3:\n",
    "        pros_ul = pros_h3.find_next('ul')\n",
    "        if pros_ul:\n",
    "            pros_text = [li.text.strip() for li in pros_ul.find_all('li')]\n",
    "            reviews.append({\n",
    "                'reviewer': 'Momondo Aggregated',\n",
    "                'date': 'N/A',\n",
    "                'rating': 'N/A',\n",
    "                'title': 'Pros of Business Hotel',\n",
    "                'text': '; '.join(pros_text),\n",
    "                'sentiment_label': 'Positive' # Can manually label these\n",
    "            })\n",
    "\n",
    "    cons_text = []\n",
    "    cons_h3 = soup.find('h3', string=lambda text: text and 'Cons -' in text.strip())\n",
    "    if cons_h3:\n",
    "        cons_ul = cons_h3.find_next('ul')\n",
    "        if cons_ul:\n",
    "            cons_text = [li.text.strip() for li in cons_ul.find_all('li')]\n",
    "            reviews.append({\n",
    "                'reviewer': 'Momondo Aggregated',\n",
    "                'date': 'N/A',\n",
    "                'rating': 'N/A',\n",
    "                'title': 'Cons of Business Hotel',\n",
    "                'text': '; '.join(cons_text),\n",
    "                'sentiment_label': 'Negative' # Can manually label these\n",
    "            })\n",
    "    \n",
    "    if not reviews:\n",
    "        reviews.append({'reviewer': 'N/A', 'date': 'N/A', 'rating': 'N/A', 'title': 'No Data', 'text': 'No pros/cons found', 'sentiment_label': 'Neutral'})\n",
    "    \n",
    "    return reviews\n",
    "\n",
    "\n",
    "# Function for Selenium scraping (dynamic sites)\n",
    "def scrape_with_selenium(url, source, review_locator, parse_func, pages=2):\n",
    "    reviews = []\n",
    "    options = Options()\n",
    "    options.headless = True # Run in headless mode\n",
    "    # Fix for newer Chrome versions:\n",
    "    options.add_argument(\"--disable-gpu\")\n",
    "    options.add_argument(\"--no-sandbox\")\n",
    "    options.add_argument(\"--disable-dev-shm-usage\")\n",
    "    options.add_argument(f\"user-agent={HEADERS['User-Agent']}\") # Pass User-Agent to Selenium\n",
    "\n",
    "    driver = None\n",
    "    try:\n",
    "        driver = webdriver.Chrome(service=Service(ChromeDriverManager().install()), options=options)\n",
    "        driver.get(url)\n",
    "        print(f\"Initial page load for {source}: {url}\")\n",
    "        time.sleep(5)  # Longer initial load for stability\n",
    "\n",
    "        # --- Specific logic for Google Maps & TripAdvisor ---\n",
    "        if source == 'Google Maps':\n",
    "            # This is the reviews panel, it's often more reliable to find it by its ARIA role or data-attributes\n",
    "            # Or by its parent container that is scrollable\n",
    "            # This CSS selector (.m6QErb.DxyBCb.kA9KIf.dS8AEf.XiKgde) might be volatile.\n",
    "            # Look for a more stable element, e.g., the div containing all reviews, which has a scrollbar.\n",
    "            try:\n",
    "                # Find the scrollable reviews panel\n",
    "                # This selector is crucial and needs to be verified on Google Maps\n",
    "                reviews_panel = WebDriverWait(driver, 20).until(\n",
    "                    EC.presence_of_element_located((By.CSS_SELECTOR, 'div.m6QErb.DxyBCb.kA9KIf.dS8AEf.XiKgde')) # Verify this\n",
    "                )\n",
    "                print(\"Google Maps reviews panel found.\")\n",
    "                # Scroll multiple times to load all reviews\n",
    "                for _ in range(20): # Increased scrolls for more reviews\n",
    "                    driver.execute_script(\"arguments[0].scrollBy(0, 5000);\", reviews_panel) # Scroll more aggressively\n",
    "                    time.sleep(2) # Increased sleep for more content to load\n",
    "                    # Optional: check if new reviews loaded to stop early\n",
    "                print(\"Finished scrolling Google Maps reviews.\")\n",
    "            except Exception as e:\n",
    "                print(f\"Could not find or scroll Google Maps reviews panel: {e}\")\n",
    "\n",
    "        # TripAdvisor pagination is handled internally by Selenium loop\n",
    "        \n",
    "        # --- Extract reviews after loading/scrolling ---\n",
    "        review_blocks = driver.find_elements(*review_locator)\n",
    "        if not review_blocks:\n",
    "            print(f\"No review blocks found for {source} with locator {review_locator}.\")\n",
    "        \n",
    "        # This loop will now get all currently loaded reviews after scrolling/pagination\n",
    "        for block in review_blocks:\n",
    "            try:\n",
    "                review = parse_func(block)\n",
    "                review['source'] = source\n",
    "                reviews.append(review)\n",
    "            except Exception as e:\n",
    "                print(f\"Error parsing {source} review: {e} in block: {block.text[:100]}...\") # Log partial block\n",
    "\n",
    "        # TripAdvisor specific pagination (if not already handled by the above loop)\n",
    "        if source == 'TripAdvisor':\n",
    "            for page_num in range(pages): # pages here represents \"next\" clicks\n",
    "                try:\n",
    "                    next_button = driver.find_element(By.CSS_SELECTOR, '.ui_button.nav.next.primary') # Verify selector\n",
    "                    if next_button.is_displayed() and next_button.is_enabled():\n",
    "                        next_button.click()\n",
    "                        time.sleep(5) # Wait for new page to load\n",
    "                        \n",
    "                        # Re-find review blocks after navigating to next page\n",
    "                        new_review_blocks = driver.find_elements(*review_locator)\n",
    "                        if not new_review_blocks:\n",
    "                            print(f\"No new reviews found after clicking next on TripAdvisor page {page_num + 1}.\")\n",
    "                            break\n",
    "                        for block in new_review_blocks:\n",
    "                            try:\n",
    "                                review = parse_func(block)\n",
    "                                review['source'] = source\n",
    "                                reviews.append(review)\n",
    "                            except Exception as e:\n",
    "                                print(f\"Error parsing {source} review: {e} in new block: {block.text[:100]}...\")\n",
    "                    else:\n",
    "                        print(f\"No more next button or not enabled on TripAdvisor after {page_num + 1} pages.\")\n",
    "                        break\n",
    "                except Exception as e:\n",
    "                    print(f\"Error with TripAdvisor pagination on page {page_num + 1}: {e}\")\n",
    "                    break\n",
    "        \n",
    "    finally:\n",
    "        if driver:\n",
    "            driver.quit()\n",
    "    return reviews\n",
    "\n",
    "\n",
    "# Parse function for TripAdvisor (using Selenium WebElement methods)\n",
    "def parse_tripadvisor(block):\n",
    "    # These selectors are examples. YOU MUST VERIFY THEM with browser inspector.\n",
    "    try:\n",
    "        reviewer_elem = block.find_elements(By.CSS_SELECTOR, '.info_text div:first-child') # Example\n",
    "        reviewer = reviewer_elem[0].text.strip() if reviewer_elem else 'Anonymous'\n",
    "        \n",
    "        date_elem = block.find_elements(By.CSS_SELECTOR, '.ratingDate') # Example\n",
    "        date = date_elem[0].get_attribute('title') if date_elem else 'N/A' # Date is often in title attribute\n",
    "        \n",
    "        rating_elem = block.find_elements(By.CSS_SELECTOR, '.ui_bubble_rating') # Example\n",
    "        rating = 'N/A'\n",
    "        if rating_elem:\n",
    "            # Rating is often in a class like 'bubble_50' for 5 stars, 'bubble_40' for 4 stars\n",
    "            for cls in rating_elem[0].get_attribute('class').split():\n",
    "                if 'bubble_' in cls:\n",
    "                    rating = str(int(cls.replace('bubble_', '')) / 10.0) # Convert 'bubble_50' to '5.0'\n",
    "                    break\n",
    "\n",
    "        title_elem = block.find_elements(By.CSS_SELECTOR, '.noQuotes') # Example\n",
    "        title = title_elem[0].text.strip() if title_elem else 'No Title'\n",
    "        \n",
    "        text_elem = block.find_elements(By.CSS_SELECTOR, '.partial_entry') # Example, might need to click \"more\"\n",
    "        text = text_elem[0].text.strip() if text_elem else 'No Text'\n",
    "        \n",
    "        return {\n",
    "            'reviewer': reviewer,\n",
    "            'date': date,\n",
    "            'rating': rating,\n",
    "            'title': title,\n",
    "            'text': text\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"TripAdvisor parse error: {e}. Block text: {block.text[:100]}...\")\n",
    "        return {'reviewer': 'N/A', 'date': 'N/A', 'rating': 'N/A', 'title': 'No Data', 'text': 'Parse failed'}\n",
    "\n",
    "# Parse function for Google Maps (using Selenium WebElement methods)\n",
    "def parse_google(block):\n",
    "    # These selectors are examples. YOU MUST VERIFY THEM with browser inspector.\n",
    "    try:\n",
    "        # Reviewer name\n",
    "        reviewer_elem = block.find_elements(By.CSS_SELECTOR, '.d4r55') # Example\n",
    "        reviewer = reviewer_elem[0].text.strip() if reviewer_elem else 'Anonymous'\n",
    "\n",
    "        # Date\n",
    "        date_elem = block.find_elements(By.CSS_SELECTOR, '.rsqaWe') # Example\n",
    "        date = date_elem[0].text.strip() if date_elem else 'N/A'\n",
    "        \n",
    "        # Rating - often an aria-label on a star element or its parent\n",
    "        rating_elem = block.find_elements(By.CSS_SELECTOR, '.kvMYJc') # Example: the div containing stars\n",
    "        rating = 'N/A'\n",
    "        if rating_elem:\n",
    "            rating_label = rating_elem[0].get_attribute('aria-label') # \"Note 5 sur 5\"\n",
    "            if rating_label and 'sur' in rating_label:\n",
    "                rating = rating_label.split(' ')[1] # Extract '5'\n",
    "        \n",
    "        # Full review text\n",
    "        text_elem = block.find_elements(By.CSS_SELECTOR, '.wiI7pd') # Example\n",
    "        text = text_elem[0].text.strip() if text_elem else 'No Text'\n",
    "\n",
    "        return {\n",
    "            'reviewer': reviewer,\n",
    "            'date': date,\n",
    "            'rating': rating,\n",
    "            'title': 'No Title (Google Maps)', # Google Maps reviews typically don't have titles\n",
    "            'text': text\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Google Maps parse error: {e}. Block text: {block.text[:100]}...\")\n",
    "        return {'reviewer': 'N/A', 'date': 'N/A', 'rating': 'N/A', 'title': 'No Data', 'text': 'Parse failed'}\n",
    "\n",
    "\n",
    "# Save to CSV\n",
    "def save_to_csv(reviews, filename='hotel_reviews_combined__final.csv'): \n",
    "    if not reviews:\n",
    "        print(\"No reviews to save.\")\n",
    "        return\n",
    "    # Ensure all review dictionaries have the same keys for DictWriter\n",
    "    # Create a superset of all keys found across all reviews\n",
    "    all_keys = set()\n",
    "    for review in reviews:\n",
    "        all_keys.update(review.keys())\n",
    "    \n",
    "    # Fill missing keys with None for consistency\n",
    "    reviews_for_csv = []\n",
    "    for review in reviews:\n",
    "        full_review = {key: review.get(key) for key in all_keys}\n",
    "        reviews_for_csv.append(full_review)\n",
    "\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as f:\n",
    "        writer = csv.DictWriter(f, sorted(list(all_keys))) # Sort keys for consistent column order\n",
    "        writer.writeheader()\n",
    "        writer.writerows(reviews_for_csv)\n",
    "    print(f\"Saved {len(reviews)} reviews to {filename}\")\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    # URLs\n",
    "    booking_url = \"https://www.booking.com/reviews/tn/hotel/business.en-gb.html?label=gen173nr-1FCAEoggI46AdIM1gEaEaIAQGYAQe4AQfIAQzYAQHoAQH4AQKIAgGoAgO4ApvZoMoGwAIB0gIkY2Q5ZGY5ZjMtZWEyNi00NzE5LWI5NjgtMzY4N2E4N2U3M2Q32AIG4AIB&sid=32b2429f18b4624fc09d32f51f4441cc&customer_type=total&hp_nav=0&keep_landing=1&order=featuredreviews&rows=75\"\n",
    "    momondo_url = \"https://www.momondo.com/hotels/tunis-tunis-governorate/Business-Hotel-Tunis.mhd2417013.ksp\"\n",
    "    tripadvisor_url = \"https://www.tripadvisor.com/Hotel_Review-g293758-d8767447-Reviews-Business_Hotel_Tunis-Tunis_Tunis_Governorate.html\"\n",
    "    google_url = \"https://www.google.com/maps/place/Business+H%C3%B4tel/@36.818325,10.1820211,17z/data=!4m8!3m7!1s0x12fd3489b5a4f4e1:0xdda07f445b5b03cd!8m2!3d36.818325!4d10.184596!9m1!1b1!16s%2Fg%2F11bwn563xh?entry=ttu\"\n",
    "    \n",
    "    all_reviews = []\n",
    "\n",
    "    print(\"\\n--- Scraping Booking.com ---\")\n",
    "    # Increased pages for Booking (75 reviews/page * 20 pages = 1500 potential reviews)\n",
    "    booking_reviews = scrape_with_bs(booking_url, 'Booking.com', 'div.c-review-block', parse_booking, pages=20) \n",
    "    all_reviews.extend(booking_reviews)\n",
    "    \n",
    "    print(\"\\n--- Scraping Momondo.com ---\")\n",
    "    momondo_reviews = scrape_with_bs(momondo_url, 'Momondo', '', parse_momondo, pages=1) # Momondo is one-off\n",
    "    all_reviews.extend(momondo_reviews)\n",
    "    \n",
    "    print(\"\\n--- Scraping TripAdvisor.com ---\")\n",
    "    # TripAdvisor pages count how many \"next\" clicks to simulate\n",
    "    tripadvisor_reviews = scrape_with_selenium(tripadvisor_url, 'TripAdvisor', (By.CSS_SELECTOR, 'div[data-reviewid]'), parse_tripadvisor, pages=10) \n",
    "    all_reviews.extend(tripadvisor_reviews)\n",
    "\n",
    "    print(\"\\n--- Scraping Google Maps ---\")\n",
    "    # Google Maps needs more scrolls than pages, so 'pages' here could mean \"how many times to try and scroll and extract\"\n",
    "    google_reviews = scrape_with_selenium(google_url, 'Google Maps', (By.CSS_SELECTOR, 'div.jftiEf'), parse_google, pages=1) # pages here refers to how many times to execute the scroll loop\n",
    "    all_reviews.extend(google_reviews)\n",
    "    \n",
    "    save_to_csv(all_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10dd087",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML_Cours",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
